WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
INFO:transformers.tokenization_utils:loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /data3/linming/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/pytorch_model.bin
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification_textCNN not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'textcnn.convs.0.weight', 'textcnn.convs.0.bias', 'textcnn.convs.1.weight', 'textcnn.convs.1.bias', 'textcnn.convs.2.weight', 'textcnn.convs.2.bias', 'textcnn.convs.3.weight', 'textcnn.convs.3.bias', 'textcnn.convs.4.weight', 'textcnn.convs.4.bias', 'textcnn.fc.0.weight', 'textcnn.fc.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification_textCNN: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO:__main__:finish loading model
INFO:__main__:Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/no_val_mutant/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, filter_num=128, filter_size=[2, 3, 4, 5, 6], fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=200, max_steps=-1, model_name='dnabert_textcnn_mutant_lr', model_name_or_path='/data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/', model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=5.0, output_dir='/data3/linming/DNABERT/examples/output/no_val_mutant/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=64, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_train_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 19097
INFO:__main__:  Num Epochs = 5
INFO:__main__:  Instantaneous batch size per GPU = 64
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 1495
INFO:__main__:  Continuing training from checkpoint, will skip to saved global_step
INFO:__main__:  Continuing training from epoch 0
INFO:__main__:  Continuing training from global step 0
INFO:__main__:  Will skip the first 0 steps in the first epoch
WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt HTTP/1.1" 200 0
INFO:transformers.tokenization_utils:loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /data3/linming/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/pytorch_model.bin
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification_textCNN not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'textcnn.convs.0.weight', 'textcnn.convs.0.bias', 'textcnn.convs.1.weight', 'textcnn.convs.1.bias', 'textcnn.convs.2.weight', 'textcnn.convs.2.bias', 'textcnn.convs.3.weight', 'textcnn.convs.3.bias', 'textcnn.convs.4.weight', 'textcnn.convs.4.bias', 'textcnn.fc.0.weight', 'textcnn.fc.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification_textCNN: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO:__main__:finish loading model
INFO:__main__:Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/no_val_mutant/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, filter_num=128, filter_size=[2, 3, 4, 5, 6], fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=200, max_steps=-1, model_name='dnabert_textcnn_mutant_lr', model_name_or_path='/data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/', model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=5.0, output_dir='/data3/linming/DNABERT/examples/output/no_val_mutant/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=64, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_train_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 19097
INFO:__main__:  Num Epochs = 5
INFO:__main__:  Instantaneous batch size per GPU = 64
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 1495
INFO:__main__:  Continuing training from checkpoint, will skip to saved global_step
INFO:__main__:  Continuing training from epoch 0
INFO:__main__:  Continuing training from global step 0
INFO:__main__:  Will skip the first 0 steps in the first epoch
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7218543046357616
INFO:__main__:  auc = 0.7814814814814816
INFO:__main__:  f1 = 0.720862676056338
INFO:__main__:  mcc = 0.46762355309467163
INFO:__main__:  precision = 0.7376138433515482
INFO:__main__:  recall = 0.7300705467372134
INFO:__main__:{"eval_acc": 0.7218543046357616, "eval_f1": 0.720862676056338, "eval_mcc": 0.46762355309467163, "eval_auc": 0.7814814814814816, "eval_precision": 0.7376138433515482, "eval_recall": 0.7300705467372134, "learning_rate": 0.0001342281879194631, "loss": 0.7137663847208023, "step": 100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.35273368606701944
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.35273368606701944, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.0001924219910846954, "loss": 0.7091496765613556, "step": 200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.7275132275132277
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.7275132275132277, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.0001775631500742942, "loss": 0.6986905574798584, "step": 300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.7340388007054675
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.7340388007054675, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.00016270430906389303, "loss": 0.6928050827980041, "step": 400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.7483245149911817
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.7483245149911817, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.00014784546805349184, "loss": 0.6926967799663544, "step": 500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.7472663139329806
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.7472663139329806, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.00013298662704309063, "loss": 0.6938313114643097, "step": 600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.7432980599647266
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.7432980599647266, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.00011812778603268946, "loss": 0.6929321444034576, "step": 700}
WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt HTTP/1.1" 200 0
INFO:transformers.tokenization_utils:loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /data3/linming/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/pytorch_model.bin
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification_textCNN not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'textcnn.convs.0.weight', 'textcnn.convs.0.bias', 'textcnn.convs.1.weight', 'textcnn.convs.1.bias', 'textcnn.convs.2.weight', 'textcnn.convs.2.bias', 'textcnn.convs.3.weight', 'textcnn.convs.3.bias', 'textcnn.convs.4.weight', 'textcnn.convs.4.bias', 'textcnn.fc.0.weight', 'textcnn.fc.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification_textCNN: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO:__main__:finish loading model
INFO:__main__:Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/no_val_mutant/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, filter_num=128, filter_size=[2, 3, 4, 5, 6], fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=200, max_steps=-1, model_name='dnabert_textcnn_mutant_lr', model_name_or_path='/data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/', model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=15.0, output_dir='/data3/linming/DNABERT/examples/output/no_val_mutant/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=64, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_train_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 19097
INFO:__main__:  Num Epochs = 15
INFO:__main__:  Instantaneous batch size per GPU = 64
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 4485
INFO:__main__:  Continuing training from checkpoint, will skip to saved global_step
INFO:__main__:  Continuing training from epoch 0
INFO:__main__:  Continuing training from global step 0
INFO:__main__:  Will skip the first 0 steps in the first epoch
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7019867549668874
INFO:__main__:  auc = 0.7574955908289241
INFO:__main__:  f1 = 0.6994027869940278
INFO:__main__:  mcc = 0.43880311002226496
INFO:__main__:  precision = 0.7265037593984962
INFO:__main__:  recall = 0.7125220458553791
INFO:__main__:{"eval_acc": 0.7019867549668874, "eval_f1": 0.6994027869940278, "eval_mcc": 0.43880311002226496, "eval_auc": 0.7574955908289241, "eval_precision": 0.7265037593984962, "eval_recall": 0.7125220458553791, "learning_rate": 4.464285714285715e-05, "loss": 0.6934248006343842, "step": 100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6490066225165563
INFO:__main__:  auc = 0.7389770723104057
INFO:__main__:  f1 = 0.6413944526594075
INFO:__main__:  mcc = 0.2898508614636038
INFO:__main__:  precision = 0.6481208548268238
INFO:__main__:  recall = 0.6417989417989418
INFO:__main__:{"eval_acc": 0.6490066225165563, "eval_f1": 0.6413944526594075, "eval_mcc": 0.2898508614636038, "eval_auc": 0.7389770723104057, "eval_precision": 0.6481208548268238, "eval_recall": 0.6417989417989418, "learning_rate": 8.92857142857143e-05, "loss": 0.6775750958919525, "step": 200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7284768211920529
INFO:__main__:  auc = 0.7611992945326278
INFO:__main__:  f1 = 0.7253936571301841
INFO:__main__:  mcc = 0.49965859777804944
INFO:__main__:  precision = 0.7599274532264223
INFO:__main__:  recall = 0.7401234567901235
INFO:__main__:{"eval_acc": 0.7284768211920529, "eval_f1": 0.7253936571301841, "eval_mcc": 0.49965859777804944, "eval_auc": 0.7611992945326278, "eval_precision": 0.7599274532264223, "eval_recall": 0.7401234567901235, "learning_rate": 0.00013392857142857144, "loss": 0.6943626761436462, "step": 300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/no_val_mutant/cached_dev_6-new-12w-0_200_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 151
INFO:__main__:  Batch size = 64
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5364238410596026
INFO:__main__:  auc = 0.5493827160493827
INFO:__main__:  f1 = 0.34913793103448276
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2682119205298013
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5364238410596026, "eval_f1": 0.34913793103448276, "eval_mcc": 0.0, "eval_auc": 0.5493827160493827, "eval_precision": 0.2682119205298013, "eval_recall": 0.5, "learning_rate": 0.0001785714285714286, "loss": 1.2655191969871522, "step": 400}

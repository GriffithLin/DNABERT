2023-06-27 14:13:48,722 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
2023-06-27 14:14:01,124 - INFO - __main__ -   finish loading model
2023-06-27 14:14:01,185 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/fold5_100_15296/3/after/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=15, eval_all_checkpoints=False, eval_batch_size=48, evaluate_during_training=True, filter_num=128, filter_size=[2, 3, 4, 5, 6], fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=300, max_steps=-1, model_name='mutant_Bert_fold5_100_15296_0.2_fold3', model_name_or_path='/data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/', model_num=5, model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=30.0, output_dir='/data3/linming/DNABERT/examples/output/testing/_fold3', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=48, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=48, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', train_batch_size=48, visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
2023-06-27 14:14:01,186 - INFO - __main__ -   Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_15296/3/after/cached_train_6-new-12w-0_300_dnaprom
2023-06-27 14:14:03,561 - INFO - __main__ -   ***** Running training *****
2023-06-27 14:14:03,561 - INFO - __main__ -     Num examples = 12238
2023-06-27 14:14:03,561 - INFO - __main__ -     Num Epochs = 30
2023-06-27 14:14:03,561 - INFO - __main__ -     Instantaneous batch size per GPU = 48
2023-06-27 14:14:03,561 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 48
2023-06-27 14:14:03,561 - INFO - __main__ -     Gradient Accumulation steps = 1
2023-06-27 14:14:03,561 - INFO - __main__ -     Total optimization steps = 7650
2023-06-27 14:14:03,561 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
2023-06-27 14:14:03,561 - INFO - __main__ -     Continuing training from epoch 0
2023-06-27 14:14:03,561 - INFO - __main__ -     Continuing training from global step 0
2023-06-27 14:14:03,561 - INFO - __main__ -     Will skip the first 0 steps in the first epoch

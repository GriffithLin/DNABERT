WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
INFO:transformers.tokenization_utils:loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /data3/linming/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/pytorch_model.bin
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO:__main__:finish loading model
INFO:__main__:Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0001, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=300, max_steps=-1, model_name='mutant_bert_fold_100_1_13944', model_name_or_path='/data3/linming/DNABERT/examples/embeding_model/6-new-12w-0/', model_num=3, model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=15.0, output_dir='/data3/linming/DNABERT/examples/output/fold5_100_13944/1/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=32, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=47, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_train_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 13486
INFO:__main__:  Num Epochs = 15
INFO:__main__:  Instantaneous batch size per GPU = 32
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 6330
INFO:__main__:  Continuing training from checkpoint, will skip to saved global_step
INFO:__main__:  Continuing training from epoch 0
INFO:__main__:  Continuing training from global step 0
INFO:__main__:  Will skip the first 0 steps in the first epoch
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5963248369887374
INFO:__main__:  auc = 0.6786788338953873
INFO:__main__:  f1 = 0.5436809869560382
INFO:__main__:  mcc = 0.29784834734464216
INFO:__main__:  precision = 0.7100314784081618
INFO:__main__:  recall = 0.6055956453388551
INFO:__main__:{"eval_acc": 0.5963248369887374, "eval_f1": 0.5436809869560382, "eval_mcc": 0.29784834734464216, "eval_auc": 0.6786788338953873, "eval_precision": 0.7100314784081618, "eval_recall": 0.6055956453388551, "learning_rate": 1.579778830963665e-05, "loss": 0.6681683868169784, "step": 100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6680497925311203
INFO:__main__:  auc = 0.7304602302434606
INFO:__main__:  f1 = 0.6530295640318968
INFO:__main__:  mcc = 0.3877199010059693
INFO:__main__:  precision = 0.7160675497597024
INFO:__main__:  recall = 0.673934866437907
INFO:__main__:{"eval_acc": 0.6680497925311203, "eval_f1": 0.6530295640318968, "eval_mcc": 0.3877199010059693, "eval_auc": 0.7304602302434606, "eval_precision": 0.7160675497597024, "eval_recall": 0.673934866437907, "learning_rate": 3.15955766192733e-05, "loss": 0.6361006522178649, "step": 200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6754593953764079
INFO:__main__:  auc = 0.7444660452252136
INFO:__main__:  f1 = 0.6662319752501433
INFO:__main__:  mcc = 0.38607605816087454
INFO:__main__:  precision = 0.7067232163306914
INFO:__main__:  recall = 0.6802588085299994
INFO:__main__:{"eval_acc": 0.6754593953764079, "eval_f1": 0.6662319752501433, "eval_mcc": 0.38607605816087454, "eval_auc": 0.7444660452252136, "eval_precision": 0.7067232163306914, "eval_recall": 0.6802588085299994, "learning_rate": 4.739336492890996e-05, "loss": 0.6097537896037102, "step": 300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6813870776526378
INFO:__main__:  auc = 0.7358907227869719
INFO:__main__:  f1 = 0.6813601788294124
INFO:__main__:  mcc = 0.3644178484675712
INFO:__main__:  precision = 0.6823272110702084
INFO:__main__:  recall = 0.6820907141372832
INFO:__main__:{"eval_acc": 0.6813870776526378, "eval_f1": 0.6813601788294124, "eval_mcc": 0.3644178484675712, "eval_auc": 0.7358907227869719, "eval_precision": 0.6823272110702084, "eval_recall": 0.6820907141372832, "learning_rate": 6.31911532385466e-05, "loss": 0.6214466619491578, "step": 400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6935388263189093
INFO:__main__:  auc = 0.7400833604954653
INFO:__main__:  f1 = 0.6906149645645927
INFO:__main__:  mcc = 0.40287960303379633
INFO:__main__:  precision = 0.706453167675253
INFO:__main__:  recall = 0.6965481764803714
INFO:__main__:{"eval_acc": 0.6935388263189093, "eval_f1": 0.6906149645645927, "eval_mcc": 0.40287960303379633, "eval_auc": 0.7400833604954653, "eval_precision": 0.706453167675253, "eval_recall": 0.6965481764803714, "learning_rate": 7.898894154818326e-05, "loss": 0.5989988824725151, "step": 500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6781268524007114
INFO:__main__:  auc = 0.7545562938264078
INFO:__main__:  f1 = 0.6747059700856521
INFO:__main__:  mcc = 0.3574789474945928
INFO:__main__:  precision = 0.6816124061399067
INFO:__main__:  recall = 0.6759119883630043
INFO:__main__:{"eval_acc": 0.6781268524007114, "eval_f1": 0.6747059700856521, "eval_mcc": 0.3574789474945928, "eval_auc": 0.7545562938264078, "eval_precision": 0.6816124061399067, "eval_recall": 0.6759119883630043, "learning_rate": 9.478672985781992e-05, "loss": 0.6054207390546799, "step": 600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6828689982216953
INFO:__main__:  auc = 0.7494902485308715
INFO:__main__:  f1 = 0.6789664694083638
INFO:__main__:  mcc = 0.38415923192798107
INFO:__main__:  precision = 0.6981327113901574
INFO:__main__:  recall = 0.6862114469135907
INFO:__main__:{"eval_acc": 0.6828689982216953, "eval_f1": 0.6789664694083638, "eval_mcc": 0.38415923192798107, "eval_auc": 0.7494902485308715, "eval_precision": 0.6981327113901574, "eval_recall": 0.6862114469135907, "learning_rate": 9.882394242583817e-05, "loss": 0.6074161013960838, "step": 700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6790160047421459
INFO:__main__:  auc = 0.7506445951928123
INFO:__main__:  f1 = 0.6694147269432316
INFO:__main__:  mcc = 0.395298785741838
INFO:__main__:  precision = 0.7123965814631104
INFO:__main__:  recall = 0.6839261358781702
INFO:__main__:{"eval_acc": 0.6790160047421459, "eval_f1": 0.6694147269432316, "eval_mcc": 0.395298785741838, "eval_auc": 0.7506445951928123, "eval_precision": 0.7123965814631104, "eval_recall": 0.6839261358781702, "learning_rate": 9.706863261365632e-05, "loss": 0.5961215198040009, "step": 800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6828689982216953
INFO:__main__:  auc = 0.7582478824964408
INFO:__main__:  f1 = 0.6776330264481325
INFO:__main__:  mcc = 0.38885799676894583
INFO:__main__:  precision = 0.7025212366568713
INFO:__main__:  recall = 0.686660105561363
INFO:__main__:{"eval_acc": 0.6828689982216953, "eval_f1": 0.6776330264481325, "eval_mcc": 0.38885799676894583, "eval_auc": 0.7582478824964408, "eval_precision": 0.7025212366568713, "eval_recall": 0.686660105561363, "learning_rate": 9.531332280147448e-05, "loss": 0.5938611370325089, "step": 900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7065797273266153
INFO:__main__:  auc = 0.7800507237433602
INFO:__main__:  f1 = 0.7013137238701148
INFO:__main__:  mcc = 0.4400387516555657
INFO:__main__:  precision = 0.7299406084323352
INFO:__main__:  recall = 0.7105262140066588
INFO:__main__:{"eval_acc": 0.7065797273266153, "eval_f1": 0.7013137238701148, "eval_mcc": 0.4400387516555657, "eval_auc": 0.7800507237433602, "eval_precision": 0.7299406084323352, "eval_recall": 0.7105262140066588, "learning_rate": 9.355801298929261e-05, "loss": 0.5317716053128243, "step": 1000}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6647895672791938
INFO:__main__:  auc = 0.7754927597534909
INFO:__main__:  f1 = 0.6483743970823139
INFO:__main__:  mcc = 0.3491780765039238
INFO:__main__:  precision = 0.690982285008398
INFO:__main__:  recall = 0.6596029300644543
INFO:__main__:{"eval_acc": 0.6647895672791938, "eval_f1": 0.6483743970823139, "eval_mcc": 0.3491780765039238, "eval_auc": 0.7754927597534909, "eval_precision": 0.690982285008398, "eval_recall": 0.6596029300644543, "learning_rate": 9.180270317711076e-05, "loss": 0.5477087354660034, "step": 1100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7036158861885004
INFO:__main__:  auc = 0.7748950170409415
INFO:__main__:  f1 = 0.7032651388427165
INFO:__main__:  mcc = 0.4117007144852972
INFO:__main__:  precision = 0.7067121358942747
INFO:__main__:  recall = 0.7049921713285324
INFO:__main__:{"eval_acc": 0.7036158861885004, "eval_f1": 0.7032651388427165, "eval_mcc": 0.4117007144852972, "eval_auc": 0.7748950170409415, "eval_precision": 0.7067121358942747, "eval_recall": 0.7049921713285324, "learning_rate": 9.004739336492891e-05, "loss": 0.5568580701947212, "step": 1200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7053941908713693
INFO:__main__:  auc = 0.7835557815257419
INFO:__main__:  f1 = 0.7049425801472615
INFO:__main__:  mcc = 0.4159198050842101
INFO:__main__:  precision = 0.7090298270723941
INFO:__main__:  recall = 0.7068954544479618
INFO:__main__:{"eval_acc": 0.7053941908713693, "eval_f1": 0.7049425801472615, "eval_mcc": 0.4159198050842101, "eval_auc": 0.7835557815257419, "eval_precision": 0.7090298270723941, "eval_recall": 0.7068954544479618, "learning_rate": 8.829208355274707e-05, "loss": 0.505785897821188, "step": 1300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7101363366923533
INFO:__main__:  auc = 0.7827440820834358
INFO:__main__:  f1 = 0.7100776588682125
INFO:__main__:  mcc = 0.42035501250245033
INFO:__main__:  precision = 0.7101065969817492
INFO:__main__:  recall = 0.7102484394520036
INFO:__main__:{"eval_acc": 0.7101363366923533, "eval_f1": 0.7100776588682125, "eval_mcc": 0.42035501250245033, "eval_auc": 0.7827440820834358, "eval_precision": 0.7101065969817492, "eval_recall": 0.7102484394520036, "learning_rate": 8.653677374056522e-05, "loss": 0.4739991721510887, "step": 1400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6807943094250148
INFO:__main__:  auc = 0.7735033313607824
INFO:__main__:  f1 = 0.6749871539447116
INFO:__main__:  mcc = 0.3662352149422296
INFO:__main__:  precision = 0.688627104226361
INFO:__main__:  recall = 0.6777690343255511
INFO:__main__:{"eval_acc": 0.6807943094250148, "eval_f1": 0.6749871539447116, "eval_mcc": 0.3662352149422296, "eval_auc": 0.7735033313607824, "eval_precision": 0.688627104226361, "eval_recall": 0.6777690343255511, "learning_rate": 8.478146392838337e-05, "loss": 0.45428484693169596, "step": 1500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7092471843509188
INFO:__main__:  auc = 0.774226248429607
INFO:__main__:  f1 = 0.7088082176776669
INFO:__main__:  mcc = 0.4236287501156903
INFO:__main__:  precision = 0.712890858097625
INFO:__main__:  recall = 0.7107433352566584
INFO:__main__:{"eval_acc": 0.7092471843509188, "eval_f1": 0.7088082176776669, "eval_mcc": 0.4236287501156903, "eval_auc": 0.774226248429607, "eval_precision": 0.712890858097625, "eval_recall": 0.7107433352566584, "learning_rate": 8.302615411620152e-05, "loss": 0.44049527660012244, "step": 1600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6867219917012448
INFO:__main__:  auc = 0.7785693766563188
INFO:__main__:  f1 = 0.6804416644332731
INFO:__main__:  mcc = 0.3794089527280366
INFO:__main__:  precision = 0.6960860699462945
INFO:__main__:  recall = 0.6835305708478066
INFO:__main__:{"eval_acc": 0.6867219917012448, "eval_f1": 0.6804416644332731, "eval_mcc": 0.3794089527280366, "eval_auc": 0.7785693766563188, "eval_precision": 0.6960860699462945, "eval_recall": 0.6835305708478066, "learning_rate": 8.127084430401967e-05, "loss": 0.4465510165691376, "step": 1700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7092471843509188
INFO:__main__:  auc = 0.7749062686684719
INFO:__main__:  f1 = 0.7091858480557222
INFO:__main__:  mcc = 0.4185612937134494
INFO:__main__:  precision = 0.7092103709700541
INFO:__main__:  recall = 0.7093509463497787
INFO:__main__:{"eval_acc": 0.7092471843509188, "eval_f1": 0.7091858480557222, "eval_mcc": 0.4185612937134494, "eval_auc": 0.7749062686684719, "eval_precision": 0.7092103709700541, "eval_recall": 0.7093509463497787, "learning_rate": 7.951553449183782e-05, "loss": 0.3352064011991024, "step": 1800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7157676348547718
INFO:__main__:  auc = 0.7863036399366674
INFO:__main__:  f1 = 0.7154814886919036
INFO:__main__:  mcc = 0.43576656935478025
INFO:__main__:  precision = 0.718686950262967
INFO:__main__:  recall = 0.7170825725299249
INFO:__main__:{"eval_acc": 0.7157676348547718, "eval_f1": 0.7154814886919036, "eval_mcc": 0.43576656935478025, "eval_auc": 0.7863036399366674, "eval_precision": 0.718686950262967, "eval_recall": 0.7170825725299249, "learning_rate": 7.776022467965596e-05, "loss": 0.3414069926738739, "step": 1900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7095435684647303
INFO:__main__:  auc = 0.7895467457656082
INFO:__main__:  f1 = 0.7058910384700912
INFO:__main__:  mcc = 0.43956593342129757
INFO:__main__:  precision = 0.7268510385009714
INFO:__main__:  recall = 0.712935117138233
INFO:__main__:{"eval_acc": 0.7095435684647303, "eval_f1": 0.7058910384700912, "eval_mcc": 0.43956593342129757, "eval_auc": 0.7895467457656082, "eval_precision": 0.7268510385009714, "eval_recall": 0.712935117138233, "learning_rate": 7.600491486747411e-05, "loss": 0.3435164626687765, "step": 2000}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7172495554238293
INFO:__main__:  auc = 0.7927302531299744
INFO:__main__:  f1 = 0.7153999348534893
INFO:__main__:  mcc = 0.44745410413279285
INFO:__main__:  precision = 0.7276809075227422
INFO:__main__:  recall = 0.719841858375061
INFO:__main__:{"eval_acc": 0.7172495554238293, "eval_f1": 0.7153999348534893, "eval_mcc": 0.44745410413279285, "eval_auc": 0.7927302531299744, "eval_precision": 0.7276809075227422, "eval_recall": 0.719841858375061, "learning_rate": 7.424960505529226e-05, "loss": 0.3510149252414703, "step": 2100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7074688796680498
INFO:__main__:  auc = 0.7861146477554938
INFO:__main__:  f1 = 0.7071403865956059
INFO:__main__:  mcc = 0.41932693192902387
INFO:__main__:  precision = 0.7105057207264684
INFO:__main__:  recall = 0.7088245811493749
INFO:__main__:{"eval_acc": 0.7074688796680498, "eval_f1": 0.7071403865956059, "eval_mcc": 0.41932693192902387, "eval_auc": 0.7861146477554938, "eval_precision": 0.7105057207264684, "eval_recall": 0.7088245811493749, "learning_rate": 7.249429524311041e-05, "loss": 0.23914620019495486, "step": 2200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6967990515708358
INFO:__main__:  auc = 0.7748732170126015
INFO:__main__:  f1 = 0.6933661098677227
INFO:__main__:  mcc = 0.3957627518439785
INFO:__main__:  precision = 0.7013093158660845
INFO:__main__:  recall = 0.6945118077040597
INFO:__main__:{"eval_acc": 0.6967990515708358, "eval_f1": 0.6933661098677227, "eval_mcc": 0.3957627518439785, "eval_auc": 0.7748732170126015, "eval_precision": 0.7013093158660845, "eval_recall": 0.6945118077040597, "learning_rate": 7.073898543092856e-05, "loss": 0.22118011239916086, "step": 2300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7190278601066983
INFO:__main__:  auc = 0.7831245277393055
INFO:__main__:  f1 = 0.7189136860665358
INFO:__main__:  mcc = 0.4378779284897653
INFO:__main__:  precision = 0.7188866353435184
INFO:__main__:  recall = 0.7189913056564392
INFO:__main__:{"eval_acc": 0.7190278601066983, "eval_f1": 0.7189136860665358, "eval_mcc": 0.4378779284897653, "eval_auc": 0.7831245277393055, "eval_precision": 0.7188866353435184, "eval_recall": 0.7189913056564392, "learning_rate": 6.898367561874671e-05, "loss": 0.2360695966333151, "step": 2400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7181387077652638
INFO:__main__:  auc = 0.7762557607453923
INFO:__main__:  f1 = 0.7177387920403226
INFO:__main__:  mcc = 0.4356698415683319
INFO:__main__:  precision = 0.7180248531214337
INFO:__main__:  recall = 0.717645153906442
INFO:__main__:{"eval_acc": 0.7181387077652638, "eval_f1": 0.7177387920403226, "eval_mcc": 0.4356698415683319, "eval_auc": 0.7762557607453923, "eval_precision": 0.7180248531214337, "eval_recall": 0.717645153906442, "learning_rate": 6.722836580656486e-05, "loss": 0.23704582076519728, "step": 2500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7216953171310018
INFO:__main__:  auc = 0.7726283415136181
INFO:__main__:  f1 = 0.7216213444846963
INFO:__main__:  mcc = 0.44584098830977253
INFO:__main__:  precision = 0.7232293714617752
INFO:__main__:  recall = 0.7226120442343672
INFO:__main__:{"eval_acc": 0.7216953171310018, "eval_f1": 0.7216213444846963, "eval_mcc": 0.44584098830977253, "eval_auc": 0.7726283415136181, "eval_precision": 0.7232293714617752, "eval_recall": 0.7226120442343672, "learning_rate": 6.547305599438301e-05, "loss": 0.17333017208613455, "step": 2600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7148784825133373
INFO:__main__:  auc = 0.7708096214073464
INFO:__main__:  f1 = 0.7141355885510079
INFO:__main__:  mcc = 0.4291607039615743
INFO:__main__:  precision = 0.7151274833588671
INFO:__main__:  recall = 0.7140346121159635
INFO:__main__:{"eval_acc": 0.7148784825133373, "eval_f1": 0.7141355885510079, "eval_mcc": 0.4291607039615743, "eval_auc": 0.7708096214073464, "eval_precision": 0.7151274833588671, "eval_recall": 0.7140346121159635, "learning_rate": 6.371774618220117e-05, "loss": 0.1546104838140309, "step": 2700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7030231179608772
INFO:__main__:  auc = 0.7628694885045286
INFO:__main__:  f1 = 0.7029996373531842
INFO:__main__:  mcc = 0.4064149301625805
INFO:__main__:  precision = 0.7031453316901946
INFO:__main__:  recall = 0.7032696174763091
INFO:__main__:{"eval_acc": 0.7030231179608772, "eval_f1": 0.7029996373531842, "eval_mcc": 0.4064149301625805, "eval_auc": 0.7628694885045286, "eval_precision": 0.7031453316901946, "eval_recall": 0.7032696174763091, "learning_rate": 6.196243637001932e-05, "loss": 0.16417371850460769, "step": 2800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7139893301719028
INFO:__main__:  auc = 0.7792951066320257
INFO:__main__:  f1 = 0.7127792507265623
INFO:__main__:  mcc = 0.43754154265760664
INFO:__main__:  precision = 0.7214034140017287
INFO:__main__:  recall = 0.7161694326331656
INFO:__main__:{"eval_acc": 0.7139893301719028, "eval_f1": 0.7127792507265623, "eval_mcc": 0.43754154265760664, "eval_auc": 0.7792951066320257, "eval_precision": 0.7214034140017287, "eval_recall": 0.7161694326331656, "learning_rate": 6.0207126557837453e-05, "loss": 0.1607110343594104, "step": 2900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7205097806757558
INFO:__main__:  auc = 0.785133646480192
INFO:__main__:  f1 = 0.7200208201429164
INFO:__main__:  mcc = 0.4467250343725942
INFO:__main__:  precision = 0.7246413613757156
INFO:__main__:  recall = 0.7220909532343682
INFO:__main__:{"eval_acc": 0.7205097806757558, "eval_f1": 0.7200208201429164, "eval_mcc": 0.4467250343725942, "eval_auc": 0.785133646480192, "eval_precision": 0.7246413613757156, "eval_recall": 0.7220909532343682, "learning_rate": 5.8451816745655604e-05, "loss": 0.16543554312549533, "step": 3000}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7172495554238293
INFO:__main__:  auc = 0.7701610705642303
INFO:__main__:  f1 = 0.7169376463548861
INFO:__main__:  mcc = 0.4389403534710097
INFO:__main__:  precision = 0.7203396047607893
INFO:__main__:  recall = 0.7186041793467235
INFO:__main__:{"eval_acc": 0.7172495554238293, "eval_f1": 0.7169376463548861, "eval_mcc": 0.4389403534710097, "eval_auc": 0.7701610705642303, "eval_precision": 0.7203396047607893, "eval_recall": 0.7186041793467235, "learning_rate": 5.6696506933473755e-05, "loss": 0.13721569007262588, "step": 3100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7128037937166568
INFO:__main__:  auc = 0.7696760199336646
INFO:__main__:  f1 = 0.7121483576929121
INFO:__main__:  mcc = 0.4321252514385271
INFO:__main__:  precision = 0.7176017067387936
INFO:__main__:  recall = 0.7145344305076629
INFO:__main__:{"eval_acc": 0.7128037937166568, "eval_f1": 0.7121483576929121, "eval_mcc": 0.4321252514385271, "eval_auc": 0.7696760199336646, "eval_precision": 0.7176017067387936, "eval_recall": 0.7145344305076629, "learning_rate": 5.4941197121291906e-05, "loss": 0.11842414889950305, "step": 3200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7240663900414938
INFO:__main__:  auc = 0.7794986907676529
INFO:__main__:  f1 = 0.7239876150936149
INFO:__main__:  mcc = 0.44809753167562094
INFO:__main__:  precision = 0.7239791974137326
INFO:__main__:  recall = 0.7241183558699917
INFO:__main__:{"eval_acc": 0.7240663900414938, "eval_f1": 0.7239876150936149, "eval_mcc": 0.44809753167562094, "eval_auc": 0.7794986907676529, "eval_precision": 0.7239791974137326, "eval_recall": 0.7241183558699917, "learning_rate": 5.3185887309110064e-05, "loss": 0.12273739044554531, "step": 3300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7208061647895673
INFO:__main__:  auc = 0.7765351175601689
INFO:__main__:  f1 = 0.7195701315545674
INFO:__main__:  mcc = 0.45164292272264267
INFO:__main__:  precision = 0.72864828622873
INFO:__main__:  recall = 0.7230295850997509
INFO:__main__:{"eval_acc": 0.7208061647895673, "eval_f1": 0.7195701315545674, "eval_mcc": 0.45164292272264267, "eval_auc": 0.7765351175601689, "eval_precision": 0.72864828622873, "eval_recall": 0.7230295850997509, "learning_rate": 5.1430577496928215e-05, "loss": 0.12781624239869416, "step": 3400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7187314759928868
INFO:__main__:  auc = 0.761853677506555
INFO:__main__:  f1 = 0.7184691083045882
INFO:__main__:  mcc = 0.44156484468223817
INFO:__main__:  precision = 0.7215500148706255
INFO:__main__:  recall = 0.7200174892485425
INFO:__main__:{"eval_acc": 0.7187314759928868, "eval_f1": 0.7184691083045882, "eval_mcc": 0.44156484468223817, "eval_auc": 0.761853677506555, "eval_precision": 0.7215500148706255, "eval_recall": 0.7200174892485425, "learning_rate": 4.9675267684746366e-05, "loss": 0.09655499857850373, "step": 3500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7181387077652638
INFO:__main__:  auc = 0.7597098908486645
INFO:__main__:  f1 = 0.7181117418783292
INFO:__main__:  mcc = 0.4380732996227387
INFO:__main__:  precision = 0.7191905747806369
INFO:__main__:  recall = 0.7188828329347796
INFO:__main__:{"eval_acc": 0.7181387077652638, "eval_f1": 0.7181117418783292, "eval_mcc": 0.4380732996227387, "eval_auc": 0.7597098908486645, "eval_precision": 0.7191905747806369, "eval_recall": 0.7188828329347796, "learning_rate": 4.791995787256451e-05, "loss": 0.08678026353009045, "step": 3600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7095435684647303
INFO:__main__:  auc = 0.7720627714235384
INFO:__main__:  f1 = 0.7078880784779351
INFO:__main__:  mcc = 0.4306100374962671
INFO:__main__:  precision = 0.7186704459109267
INFO:__main__:  recall = 0.7119913868791256
INFO:__main__:{"eval_acc": 0.7095435684647303, "eval_f1": 0.7078880784779351, "eval_mcc": 0.4306100374962671, "eval_auc": 0.7720627714235384, "eval_precision": 0.7186704459109267, "eval_recall": 0.7119913868791256, "learning_rate": 4.616464806038266e-05, "loss": 0.10569023085292428, "step": 3700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7128037937166568
INFO:__main__:  auc = 0.7713428430682767
INFO:__main__:  f1 = 0.709440551889526
INFO:__main__:  mcc = 0.44519750949046755
INFO:__main__:  precision = 0.7292961288204394
INFO:__main__:  recall = 0.716097000280939
INFO:__main__:{"eval_acc": 0.7128037937166568, "eval_f1": 0.709440551889526, "eval_mcc": 0.44519750949046755, "eval_auc": 0.7713428430682767, "eval_precision": 0.7292961288204394, "eval_recall": 0.716097000280939, "learning_rate": 4.440933824820081e-05, "loss": 0.1055042632529512, "step": 3800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7148784825133373
INFO:__main__:  auc = 0.7659429408871136
INFO:__main__:  f1 = 0.714235857624872
INFO:__main__:  mcc = 0.43625963328431117
INFO:__main__:  precision = 0.7196676024737256
INFO:__main__:  recall = 0.716602796099764
INFO:__main__:{"eval_acc": 0.7148784825133373, "eval_f1": 0.714235857624872, "eval_mcc": 0.43625963328431117, "eval_auc": 0.7659429408871136, "eval_precision": 0.7196676024737256, "eval_recall": 0.716602796099764, "learning_rate": 4.265402843601896e-05, "loss": 0.08249190361704678, "step": 3900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7122110254890338
INFO:__main__:  auc = 0.7703715111603839
INFO:__main__:  f1 = 0.7121925948948915
INFO:__main__:  mcc = 0.4248509724963779
INFO:__main__:  precision = 0.712364004385595
INFO:__main__:  recall = 0.712486985910501
INFO:__main__:{"eval_acc": 0.7122110254890338, "eval_f1": 0.7121925948948915, "eval_mcc": 0.4248509724963779, "eval_auc": 0.7703715111603839, "eval_precision": 0.712364004385595, "eval_recall": 0.712486985910501, "learning_rate": 4.089871862383711e-05, "loss": 0.0668769712583162, "step": 4000}
INFO:transformers.configuration_utils:Configuration saved in /data3/linming/DNABERT/examples/output/fold5_100_13944/1/checkpoint-4000/config.json
INFO:transformers.modeling_utils:Model weights saved in /data3/linming/DNABERT/examples/output/fold5_100_13944/1/checkpoint-4000/pytorch_model.bin
INFO:__main__:Saving model checkpoint to /data3/linming/DNABERT/examples/output/fold5_100_13944/1/checkpoint-4000
INFO:__main__:Saving optimizer and scheduler states to /data3/linming/DNABERT/examples/output/fold5_100_13944/1/checkpoint-4000
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7157676348547718
INFO:__main__:  auc = 0.7649297669893423
INFO:__main__:  f1 = 0.7157634152038523
INFO:__main__:  mcc = 0.43281079125547245
INFO:__main__:  precision = 0.7164553666910565
INFO:__main__:  recall = 0.7163554361007767
INFO:__main__:{"eval_acc": 0.7157676348547718, "eval_f1": 0.7157634152038523, "eval_mcc": 0.43281079125547245, "eval_auc": 0.7649297669893423, "eval_precision": 0.7164553666910565, "eval_recall": 0.7163554361007767, "learning_rate": 3.914340881165526e-05, "loss": 0.07824989676708355, "step": 4100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7145820983995258
INFO:__main__:  auc = 0.7598512394195145
INFO:__main__:  f1 = 0.7145710411663966
INFO:__main__:  mcc = 0.4297058927528025
INFO:__main__:  precision = 0.7147998200629779
INFO:__main__:  recall = 0.7149060858295244
INFO:__main__:{"eval_acc": 0.7145820983995258, "eval_f1": 0.7145710411663966, "eval_mcc": 0.4297058927528025, "eval_auc": 0.7598512394195145, "eval_precision": 0.7147998200629779, "eval_recall": 0.7149060858295244, "learning_rate": 3.738809899947341e-05, "loss": 0.07296653497498483, "step": 4200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7059869590989923
INFO:__main__:  auc = 0.7612147960308477
INFO:__main__:  f1 = 0.7042033252168405
INFO:__main__:  mcc = 0.42387800815559445
INFO:__main__:  precision = 0.7154406825046242
INFO:__main__:  recall = 0.7084942403973513
INFO:__main__:{"eval_acc": 0.7059869590989923, "eval_f1": 0.7042033252168405, "eval_mcc": 0.42387800815559445, "eval_auc": 0.7612147960308477, "eval_precision": 0.7154406825046242, "eval_recall": 0.7084942403973513, "learning_rate": 3.563278918729156e-05, "loss": 0.0637897279439494, "step": 4300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7104327208061648
INFO:__main__:  auc = 0.7393636079468838
INFO:__main__:  f1 = 0.709866899026772
INFO:__main__:  mcc = 0.4201994577972225
INFO:__main__:  precision = 0.7104362555309734
INFO:__main__:  recall = 0.7097637404347981
INFO:__main__:{"eval_acc": 0.7104327208061648, "eval_f1": 0.709866899026772, "eval_mcc": 0.4201994577972225, "eval_auc": 0.7393636079468838, "eval_precision": 0.7104362555309734, "eval_recall": 0.7097637404347981, "learning_rate": 3.387747937510971e-05, "loss": 0.060474076857790354, "step": 4400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7163604030823948
INFO:__main__:  auc = 0.7288129216503465
INFO:__main__:  f1 = 0.7155417771734318
INFO:__main__:  mcc = 0.43217670590730367
INFO:__main__:  precision = 0.7167306876061121
INFO:__main__:  recall = 0.7154479220177825
INFO:__main__:{"eval_acc": 0.7163604030823948, "eval_f1": 0.7155417771734318, "eval_mcc": 0.43217670590730367, "eval_auc": 0.7288129216503465, "eval_precision": 0.7167306876061121, "eval_recall": 0.7154479220177825, "learning_rate": 3.2122169562927855e-05, "loss": 0.04981620623380877, "step": 4500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7151748666271488
INFO:__main__:  auc = 0.7446378083517315
INFO:__main__:  f1 = 0.7150186308490296
INFO:__main__:  mcc = 0.4300474458934887
INFO:__main__:  precision = 0.7149968503316995
INFO:__main__:  recall = 0.7150505989206173
INFO:__main__:{"eval_acc": 0.7151748666271488, "eval_f1": 0.7150186308490296, "eval_mcc": 0.4300474458934887, "eval_auc": 0.7446378083517315, "eval_precision": 0.7149968503316995, "eval_recall": 0.7150505989206173, "learning_rate": 3.0366859750746006e-05, "loss": 0.062470173581386915, "step": 4600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7142857142857143
INFO:__main__:  auc = 0.7665034125834687
INFO:__main__:  f1 = 0.7133767885442927
INFO:__main__:  mcc = 0.43655628691986176
INFO:__main__:  precision = 0.7203337760321409
INFO:__main__:  recall = 0.716241689178712
INFO:__main__:{"eval_acc": 0.7142857142857143, "eval_f1": 0.7133767885442927, "eval_mcc": 0.43655628691986176, "eval_auc": 0.7665034125834687, "eval_precision": 0.7203337760321409, "eval_recall": 0.716241689178712, "learning_rate": 2.8611549938564157e-05, "loss": 0.0581993631250225, "step": 4700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7166567871962063
INFO:__main__:  auc = 0.7634800651047298
INFO:__main__:  f1 = 0.716049769620953
INFO:__main__:  mcc = 0.4396547775485167
INFO:__main__:  precision = 0.7213133858570783
INFO:__main__:  recall = 0.7183513693406511
INFO:__main__:{"eval_acc": 0.7166567871962063, "eval_f1": 0.716049769620953, "eval_mcc": 0.4396547775485167, "eval_auc": 0.7634800651047298, "eval_precision": 0.7213133858570783, "eval_recall": 0.7183513693406511, "learning_rate": 2.6856240126382308e-05, "loss": 0.04096983891155105, "step": 4800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7178423236514523
INFO:__main__:  auc = 0.7627828158112089
INFO:__main__:  f1 = 0.7158573972218841
INFO:__main__:  mcc = 0.44934441473460135
INFO:__main__:  precision = 0.7289105020161017
INFO:__main__:  recall = 0.7205123850531974
INFO:__main__:{"eval_acc": 0.7178423236514523, "eval_f1": 0.7158573972218841, "eval_mcc": 0.44934441473460135, "eval_auc": 0.7627828158112089, "eval_precision": 0.7289105020161017, "eval_recall": 0.7205123850531974, "learning_rate": 2.5100930314200456e-05, "loss": 0.03422163491894025, "step": 4900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7148784825133373
INFO:__main__:  auc = 0.7569975453871316
INFO:__main__:  f1 = 0.7146176646849792
INFO:__main__:  mcc = 0.4337887776098967
INFO:__main__:  precision = 0.7176371752565713
INFO:__main__:  recall = 0.7161541374519915
INFO:__main__:{"eval_acc": 0.7148784825133373, "eval_f1": 0.7146176646849792, "eval_mcc": 0.4337887776098967, "eval_auc": 0.7569975453871316, "eval_precision": 0.7176371752565713, "eval_recall": 0.7161541374519915, "learning_rate": 2.334562050201861e-05, "loss": 0.050659596620243975, "step": 5000}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7193242442205098
INFO:__main__:  auc = 0.7553024173770136
INFO:__main__:  f1 = 0.719313370700496
INFO:__main__:  mcc = 0.43919429381398023
INFO:__main__:  precision = 0.7195428475033738
INFO:__main__:  recall = 0.7196514597404461
INFO:__main__:{"eval_acc": 0.7193242442205098, "eval_f1": 0.719313370700496, "eval_mcc": 0.43919429381398023, "eval_auc": 0.7553024173770136, "eval_precision": 0.7195428475033738, "eval_recall": 0.7196514597404461, "learning_rate": 2.1590310689836757e-05, "loss": 0.043901900812634266, "step": 5100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7160640189685833
INFO:__main__:  auc = 0.7570052808810587
INFO:__main__:  f1 = 0.7160640189685834
INFO:__main__:  mcc = 0.43310292109831355
INFO:__main__:  precision = 0.7165514605491567
INFO:__main__:  recall = 0.7165514605491567
INFO:__main__:{"eval_acc": 0.7160640189685833, "eval_f1": 0.7160640189685834, "eval_mcc": 0.43310292109831355, "eval_auc": 0.7570052808810587, "eval_precision": 0.7165514605491567, "eval_recall": 0.7165514605491567, "learning_rate": 1.983500087765491e-05, "loss": 0.03219172703975346, "step": 5200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7178423236514523
INFO:__main__:  auc = 0.760852634269715
INFO:__main__:  f1 = 0.7175842170271312
INFO:__main__:  mcc = 0.4397408090294253
INFO:__main__:  precision = 0.7206233827659538
INFO:__main__:  recall = 0.7191199961463176
INFO:__main__:{"eval_acc": 0.7178423236514523, "eval_f1": 0.7175842170271312, "eval_mcc": 0.4397408090294253, "eval_auc": 0.760852634269715, "eval_precision": 0.7206233827659538, "eval_recall": 0.7191199961463176, "learning_rate": 1.8079691065473056e-05, "loss": 0.0312869531463366, "step": 5300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7154712507409603
INFO:__main__:  auc = 0.7562203040541372
INFO:__main__:  f1 = 0.7154703509536325
INFO:__main__:  mcc = 0.43205224579826323
INFO:__main__:  precision = 0.7160475461487359
INFO:__main__:  recall = 0.7160047017738542
INFO:__main__:{"eval_acc": 0.7154712507409603, "eval_f1": 0.7154703509536325, "eval_mcc": 0.43205224579826323, "eval_auc": 0.7562203040541372, "eval_precision": 0.7160475461487359, "eval_recall": 0.7160047017738542, "learning_rate": 1.6324381253291207e-05, "loss": 0.04542562359245494, "step": 5400}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7110254890337878
INFO:__main__:  auc = 0.756117808759603
INFO:__main__:  f1 = 0.7110224174712123
INFO:__main__:  mcc = 0.42327022671849285
INFO:__main__:  precision = 0.7116756432768285
INFO:__main__:  recall = 0.7115945912020007
INFO:__main__:{"eval_acc": 0.7110254890337878, "eval_f1": 0.7110224174712123, "eval_mcc": 0.42327022671849285, "eval_auc": 0.756117808759603, "eval_precision": 0.7116756432768285, "eval_recall": 0.7115945912020007, "learning_rate": 1.4569071441109356e-05, "loss": 0.02352352371701272, "step": 5500}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7131001778304683
INFO:__main__:  auc = 0.7623680878527078
INFO:__main__:  f1 = 0.7129969120668415
INFO:__main__:  mcc = 0.42889549265005933
INFO:__main__:  precision = 0.7148154485843973
INFO:__main__:  recall = 0.7140806734661658
INFO:__main__:{"eval_acc": 0.7131001778304683, "eval_f1": 0.7129969120668415, "eval_mcc": 0.42889549265005933, "eval_auc": 0.7623680878527078, "eval_precision": 0.7148154485843973, "eval_recall": 0.7140806734661658, "learning_rate": 1.2813761628927507e-05, "loss": 0.017791547836677638, "step": 5600}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7107291049199763
INFO:__main__:  auc = 0.7578635690936075
INFO:__main__:  f1 = 0.7105898896154117
INFO:__main__:  mcc = 0.4212042673398771
INFO:__main__:  precision = 0.7105637859749623
INFO:__main__:  recall = 0.7106404883487638
INFO:__main__:{"eval_acc": 0.7107291049199763, "eval_f1": 0.7105898896154117, "eval_mcc": 0.4212042673398771, "eval_auc": 0.7578635690936075, "eval_precision": 0.7105637859749623, "eval_recall": 0.7106404883487638, "learning_rate": 1.1058451816745656e-05, "loss": 0.021551760645234026, "step": 5700}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7110254890337878
INFO:__main__:  auc = 0.761407480152305
INFO:__main__:  f1 = 0.7109828113118779
INFO:__main__:  mcc = 0.42404779277627236
INFO:__main__:  precision = 0.7122213204149441
INFO:__main__:  recall = 0.7118266560198141
INFO:__main__:{"eval_acc": 0.7110254890337878, "eval_f1": 0.7109828113118779, "eval_mcc": 0.42404779277627236, "eval_auc": 0.761407480152305, "eval_precision": 0.7122213204149441, "eval_recall": 0.7118266560198141, "learning_rate": 9.303142004563806e-06, "loss": 0.02775392666400876, "step": 5800}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7128037937166568
INFO:__main__:  auc = 0.757121840710006
INFO:__main__:  f1 = 0.7127926675911094
INFO:__main__:  mcc = 0.42614774235486086
INFO:__main__:  precision = 0.7130211847728295
INFO:__main__:  recall = 0.7131265706129288
INFO:__main__:{"eval_acc": 0.7128037937166568, "eval_f1": 0.7127926675911094, "eval_mcc": 0.42614774235486086, "eval_auc": 0.757121840710006, "eval_precision": 0.7130211847728295, "eval_recall": 0.7131265706129288, "learning_rate": 7.547832192381955e-06, "loss": 0.019323333064676262, "step": 5900}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7169531713100178
INFO:__main__:  auc = 0.7629651273385365
INFO:__main__:  f1 = 0.7166124004820675
INFO:__main__:  mcc = 0.438545683024206
INFO:__main__:  precision = 0.7202033440565551
INFO:__main__:  recall = 0.7183462709469264
INFO:__main__:{"eval_acc": 0.7169531713100178, "eval_f1": 0.7166124004820675, "eval_mcc": 0.438545683024206, "eval_auc": 0.7629651273385365, "eval_precision": 0.7202033440565551, "eval_recall": 0.7183462709469264, "learning_rate": 5.792522380200106e-06, "loss": 0.02251043521464453, "step": 6000}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7154712507409603
INFO:__main__:  auc = 0.7577797093071705
INFO:__main__:  f1 = 0.715461252784299
INFO:__main__:  mcc = 0.4323902254803439
INFO:__main__:  precision = 0.7162772579914725
INFO:__main__:  recall = 0.7161129986888338
INFO:__main__:{"eval_acc": 0.7154712507409603, "eval_f1": 0.715461252784299, "eval_mcc": 0.4323902254803439, "eval_auc": 0.7577797093071705, "eval_precision": 0.7162772579914725, "eval_recall": 0.7161129986888338, "learning_rate": 4.037212568018255e-06, "loss": 0.01737711679903441, "step": 6100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7145820983995258
INFO:__main__:  auc = 0.7615702771381344
INFO:__main__:  f1 = 0.7145168709129689
INFO:__main__:  mcc = 0.4314641492768967
INFO:__main__:  precision = 0.7160014438071389
INFO:__main__:  recall = 0.7154630413922765
INFO:__main__:{"eval_acc": 0.7145820983995258, "eval_f1": 0.7145168709129689, "eval_mcc": 0.4314641492768967, "eval_auc": 0.7615702771381344, "eval_precision": 0.7160014438071389, "eval_recall": 0.7154630413922765, "learning_rate": 2.2819027558364053e-06, "loss": 0.020213994114019442, "step": 6200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7148784825133373
INFO:__main__:  auc = 0.7625734300551364
INFO:__main__:  f1 = 0.7147016477363621
INFO:__main__:  mcc = 0.4331313754175876
INFO:__main__:  precision = 0.7171178812440693
INFO:__main__:  recall = 0.7160148985613036
INFO:__main__:{"eval_acc": 0.7148784825133373, "eval_f1": 0.7147016477363621, "eval_mcc": 0.4331313754175876, "eval_auc": 0.7625734300551364, "eval_precision": 0.7171178812440693, "eval_recall": 0.7160148985613036, "learning_rate": 5.265929436545551e-07, "loss": 0.015284494919178541, "step": 6300}
INFO:__main__: global_step = 6330, average loss = 0.23032000748200437
INFO:__main__:Saving model checkpoint to /data3/linming/DNABERT/examples/output/fold5_100_13944/1/
INFO:transformers.configuration_utils:Configuration saved in /data3/linming/DNABERT/examples/output/fold5_100_13944/1/config.json
INFO:transformers.modeling_utils:Model weights saved in /data3/linming/DNABERT/examples/output/fold5_100_13944/1/pytorch_model.bin
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/pytorch_model.bin
INFO:transformers.tokenization_utils:Model name '/data3/linming/DNABERT/examples/output/fold5_100_13944/1/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/data3/linming/DNABERT/examples/output/fold5_100_13944/1/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:Didn't find file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/vocab.txt
INFO:transformers.tokenization_utils:loading file None
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name '/data3/linming/DNABERT/examples/output/fold5_100_13944/1/' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/data3/linming/DNABERT/examples/output/fold5_100_13944/1/' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:Didn't find file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/added_tokens.json. We won't load it.
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/vocab.txt
INFO:transformers.tokenization_utils:loading file None
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/tokenizer_config.json
INFO:__main__:Evaluate the following checkpoints: ['/data3/linming/DNABERT/examples/output/fold5_100_13944/1/']
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/output/fold5_100_13944/1/pytorch_model.bin
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 3374
INFO:__main__:  Batch size = 32
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.7154712507409603
INFO:__main__:  auc = 0.7625982187970393
INFO:__main__:  f1 = 0.7153111990743817
INFO:__main__:  mcc = 0.43418137343298857
INFO:__main__:  precision = 0.7176054628835724
INFO:__main__:  recall = 0.7165771283244604
WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
INFO:transformers.configuration_utils:loading configuration file /data3/linming/DNABERT/examples/model/6-new-12w-0/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": "dnaprom",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
INFO:transformers.tokenization_utils:loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /data3/linming/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
INFO:transformers.modeling_utils:loading weights file /data3/linming/DNABERT/examples/model/6-new-12w-0/pytorch_model.bin
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
INFO:__main__:finish loading model
INFO:__main__:Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/', device=device(type='cuda'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=300, max_steps=-1, model_name='mutant_bert_fold_100_1_13944', model_name_or_path='/data3/linming/DNABERT/examples/model/6-new-12w-0/', model_num=3, model_type='dna', n_gpu=1, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=15.0, output_dir='/data3/linming/DNABERT/examples/output/fold5_100_13944/1/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=48, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=48, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=47, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna6', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)
INFO:__main__:Creating features from dataset file at /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/
INFO:transformers.data.processors.glue:LOOKING AT /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/train.tsv
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-1
INFO:transformers.data.processors.glue:input_ids: 2 3631 2221 680 2705 2616 2260 833 3320 979 3903 3311 943 3757 2727 2703 2605 2215 654 2603 2205 616 2452 1604 2305 1016 4051 3902 3308 931 3711 2543 1968 3763 2752 2803 3007 3822 2986 3739 2653 2407 1423 1582 2219 670 2667 2463 1648 2484 1732 2820 3076 4097 4088 4051 3904 3313 951 3789 2855 3214 556 2212 641 2552 2004 3907 3325 1000 3987 3647 2285 934 3723 2589 2152 404 1602 2298 987 3934 3436 1441 1655 2509 1832 3217 565 2245 776 3089 55 208 818 3259 733 2920 3476 1601 2296 980 3906 3323 991 3949 3495 1677 2600 2195 575 2286 940 3747 2685 2536 1937 3640 2257 823 3277 807 3213 552 2194 570 2267 862 3435 1437 1638 2444 1569 2168 467 1856 3314 956 3812 2946 3580 2018 3963 3551 1902 3500 1700 2692 2564 2049 4087 4048 3892 3268 769 3064 4051 3901 3304 915 3645 2280 916 3649 2293 968 3860 3137 245 968 3859 3135 240 948 3779 2815 3053 4008 3731 2624 2291 958 3819 2975 3695 2480 1715 2751 2797 2983 3725 2600 2193 567 2253 806 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2
INFO:transformers.data.processors.glue:input_ids: 2 2546 1980 3812 2945 3573 1992 3858 3130 220 865 3448 1491 1853 3303 912 3633 2229 709 2823 3087 48 179 703 2800 2995 3776 2804 3012 3842 3068 4067 3967 3568 1972 3779 2815 3056 4020 3780 2819 3070 4076 4002 3707 2528 1908 3523 1792 3060 4035 3840 3060 4035 3838 3052 4003 3712 2548 1987 3840 3058 4027 3808 2930 3516 1764 2945 3576 2003 3901 3303 912 3635 2238 748 2979 3711 2541 1958 3722 2588 2147 382 1515 1950 3692 2467 1662 2539 1952 3699 2494 1772 2979 3712 2547 1981 3816 2963 3648 2289 952 3796 2883 3327 1005 4008 3731 2624 2289 952 3796 2883 3327 1005 4008 3731 2624 2289 952 3796 2882 3321 983 3920 3379 1213 744 2963 3648 2292 962 3836 3041 3960 3539 1856 3316 961 3830 3019 3869 3175 399 1584 2226 700 2788 2947 3583 2029 4006 3724 2595 2175 496 1970 3769 2776 2900 3396 1283 1021 4071 3984 3635 2238 747 2974 3692 2468 1667 2560 2035 4029 3815 2960 3635 2239 752 2994 3769 2776 2899 3391 1261 934 3724 2595 2175 496 1971 3773 2790 2956 3619 2175 494 1963 3743 2672 2484 1731 2815 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-3
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:input_ids: 2 2728 2707 2624 2291 959 3822 2988 3748 2690 2556 2017 3957 3527 1808 3122 188 740 2946 3580 2019 3966 3563 1952 3700 2497 1784 3027 3903 3311 941 3751 2701 2597 2183 528 2098 188 739 2944 3572 1985 3831 3024 3891 3261 744 2964 3649 2296 979 3903 3311 941 3751 2703 2607 2221 680 2707 2621 2280 915 3645 2279 910 3626 2203 606 2411 1440 1652 2498 1788 3044 3971 3582 2027 3997 3688 2452 1602 2300 994 3962 3546 1883 3422 1388 1441 1653 2503 1805 3109 135 526 2089 151 592 2355 1216 756 3009 3832 3025 3893 3271 781 3109 133 519 2062 44 161 629 2503 1808 3121 183 720 2866 3258 731 2910 3435 1439 1646 2475 1693 2662 2443 1567 2157 424 1684 2626 2300 996 3972 3587 2048 4084 4036 3843 3069 4072 3988 3652 2307 1023 4080 4019 3776 2801 3000 3796 2884 3331 1022 4075 4000 3700 2497 1784 3028 3908 3332 1027 4093 4071 3984 3636 2243 767 3053 4008 3729 2616 2260 836 3331 1022 4075 3999 3696 2484 1732 2817 3063 4047 3887 3247 687 2733 2726 2698 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4
INFO:transformers.data.processors.glue:input_ids: 2 1908 3522 1788 3044 3972 3586 2041 4054 3916 3364 1153 503 1997 3879 3216 562 2234 731 2909 3430 1419 1567 2157 423 1679 2608 2227 701 2790 2955 3616 2161 439 1742 2859 3231 621 2471 1679 2608 2225 696 2772 2882 3323 989 3942 3467 1566 2153 407 1615 2349 1192 659 2623 2288 947 3776 2803 3007 3824 2995 3773 2789 2952 3603 2112 244 964 3843 3071 4077 4005 3720 2579 2110 235 925 3686 2443 1568 2164 451 1789 3045 3976 3602 2105 215 847 3374 1196 673 2678 2508 1828 3201 503 1999 3886 3244 675 2686 2540 1956 3716 2564 2052 4097 4085 4040 3860 3139 254 1003 3998 3690 2457 1623 2384 1332 1219 765 3045 3976 3604 2114 252 993 3957 3528 1812 3137 248 980 3906 3324 995 3966 3564 1956 3713 2551 1998 3883 3232 628 2497 1784 3025 3895 3280 819 3262 748 2978 3708 2531 1917 3560 1940 3649 2296 980 3908 3331 1023 4080 4018 3771 2781 2917 3464 1553 2102 203 799 3182 427 1693 2661 2440 1553 2101 200 785 3125 200 785 3125 200 786 3130 220 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-1395
INFO:transformers.data.processors.glue:input_ids: 2 430 1707 2719 2670 2473 1685 2632 2322 1082 219 861 3430 1417 1557 2119 270 1066 155 606 2412 1441 1654 2507 1822 3180 420 1667 2559 2029 4006 3722 2586 2140 356 1412 1538 2042 4058 3931 3422 1386 1436 1636 2433 1528 2004 3906 3322 986 3931 3423 1389 1448 1684 2626 2300 993 3958 3530 1818 3163 350 1385 1430 1610 2332 1121 374 1482 1819 3166 364 1441 1656 2515 1855 3310 940 3746 2684 2529 1911 3534 1834 3228 611 2429 1509 1926 3596 2082 122 476 1892 3457 1525 1990 3852 3106 122 475 1886 3435 1438 1644 2468 1667 2557 2024 3988 3652 2307 1023 4078 4011 3743 2669 2470 1674 2588 2146 379 1503 1901 3496 1684 2627 2301 1000 3985 3638 2250 794 3162 348 1377 1399 1485 1831 3213 551 2190 553 2200 595 2366 1260 930 3708 2531 1917 3557 1927 3599 2094 171 671 2669 2470 1674 2587 2144 370 1468 1763 2941 3560 1940 3651 2302 1001 3991 3661 2341 1160 532 2116 257 1016 4049 3893 3272 786 3130 217 854 3401 1303 1102 300 1185 632 2513 1848 3282 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5
INFO:transformers.data.processors.glue:input_ids: 2 341 1349 1286 1033 22 74 282 1113 343 1357 1318 1163 542 2154 410 1626 2394 1371 1374 1386 1436 1634 2425 1493 1862 3338 1051 93 359 1421 1576 2196 577 2293 966 3852 3108 129 501 1989 3847 3086 44 161 629 2501 1800 3090 60 228 897 3573 1989 3846 3083 29 104 404 1601 2293 965 3845 3080 20 66 249 981 3910 3340 1060 129 501 1992 3857 3125 200 785 3125 197 776 3091 61 231 910 3628 2209 629 2501 1797 3080 17 53 197 773 3077 5 8 18 58 220 865 3445 1477 1797 3077 5 5 8 17 53 197 775 3088 52 194 763 3037 3943 3472 1585 2232 722 2874 3289 853 3397 1287 1037 40 148 578 2300 995 3966 3562 1948 3683 2429 1512 1937 3638 2249 790 3146 284 1123 382 1513 1941 3656 2323 1085 229 904 3602 2108 228 897 3575 1998 3882 3227 606 2412 1444 1666 2554 2010 3932 3428 1412 1540 2049 4086 4044 3875 3197 485 1925 3590 2059 30 106 411 1629 2407 1421 1573 2184 532 2113 246 969 3864 3153 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-1396
INFO:transformers.data.processors.glue:input_ids: 2 3796 2881 3320 980 3905 3320 977 3894 3276 803 3200 499 1983 3824 2996 3779 2815 3055 4015 3760 2740 2755 2815 3056 4019 3775 2799 2990 3756 2723 2686 2540 1955 3709 2535 1934 3627 2205 615 2447 1582 2219 669 2664 2451 1600 2291 958 3820 2978 3707 2528 1908 3521 1783 3024 3892 3268 771 3070 4075 3998 3691 2463 1645 2471 1677 2600 2195 575 2285 934 3723 2590 2154 411 1630 2412 1443 1663 2541 1959 3728 2612 2244 770 3067 4063 3952 3507 1727 2797 2984 3731 2622 2284 931 3709 2534 1931 3614 2154 411 1631 2415 1455 1711 2735 2735 2736 2740 2755 2815 3055 4016 3763 2751 2800 2995 3775 2798 2986 3739 2653 2407 1423 1584 2227 703 2798 2987 3743 2670 2475 1695 2671 2478 1707 2719 2671 2479 1712 2739 2751 2799 2992 3763 2751 2799 2991 3757 2727 2704 2612 2243 767 3056 4020 3779 2815 3055 4015 3760 2739 2750 2795 2975 3696 2482 1723 2784 2931 3519 1776 2995 3775 2799 2992 3761 2744 2771 2879 3311 943 3757 2727 2704 2612 2243 765 3048 3987 3648 2292 963 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-1397
INFO:transformers.data.processors.glue:input_ids: 2 3691 2463 1647 2479 1710 2730 2715 2655 2415 1455 1710 2731 2719 2671 2478 1707 2719 2671 2479 1711 2735 2735 2734 2731 2719 2670 2475 1695 2670 2475 1695 2670 2475 1695 2670 2475 1694 2668 2467 1663 2544 1971 3775 2799 2989 3752 2705 2616 2259 832 3313 952 3793 2871 3277 807 3215 557 2213 647 2573 2086 140 548 2177 504 2003 3903 3311 944 3761 2744 2772 2881 3318 971 3870 3180 419 1663 2541 1958 3724 2596 2179 511 2032 4020 3780 2819 3072 4081 4024 3795 2878 3308 932 3713 2552 2004 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3320 980 3905 3317 968 3857 3128 212 833 3320 980 3905 3320 980 3907 3326 1004 4003 3709 2536 1939 3648 2292 963 3840 3060 4035 3840 3060 4035 3840 3060 4035 3840 3057 4021 3783 2832 3122 188 740 2945 3575 2000 3889 3255 720 2866 3257 728 2898 3388 1252 898 3579 2016 3954 3516 1764 2945 3576 2004 3905 3320 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2789
INFO:transformers.data.processors.glue:input_ids: 2 2476 1699 2685 2535 1935 3629 2216 659 2624 2289 952 3794 2873 3287 848 3379 1216 755 3007 3824 2994 3770 2780 2916 3459 1535 2031 4013 3752 2707 2623 2288 946 3770 2779 2909 3431 1424 1586 2236 739 2944 3570 1980 3810 2937 3543 1872 3378 1212 738 2939 3551 1901 3496 1684 2627 2304 1011 4030 3820 2980 3715 2559 2031 4013 3751 2704 2611 2239 751 2989 3751 2703 2608 2228 705 2807 3022 3884 3235 638 2540 1956 3715 2560 2036 4035 3837 3048 3987 3648 2292 963 3838 3052 4003 3712 2548 1987 3840 3060 4035 3839 3056 4019 3776 2804 3011 3837 3047 3983 3632 2228 708 2817 3064 4052 3907 3325 1000 3988 3649 2296 980 3907 3327 1008 4019 3774 2796 2980 3716 2563 2045 4072 3988 3652 2305 1016 4050 3897 3287 846 3370 1177 598 2379 1310 1130 410 1627 2398 1386 1436 1633 2424 1491 1853 3304 915 3648 2290 955 3806 2922 3484 1636 2434 1531 2014 3946 3484 1635 2432 1524 1987 3840 3059 4032 3828 3010 3835 3040 3956 3522 1785 3031 3918 3370 1180 610 2425 1496 1874 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:guid: train-1398
INFO:transformers.data.processors.glue:input_ids: 2 2723 2685 2534 1932 3620 2179 511 2031 4013 3752 2705 2616 2257 824 3281 821 3269 774 3084 35 125 488 1937 3638 2252 801 3191 461 1832 3217 567 2253 806 3209 535 2127 301 1190 651 2589 2152 404 1602 2298 987 3935 3440 1458 1724 2788 2948 3588 2051 4093 4072 3985 3640 2259 830 3307 928 3699 2493 1768 2964 3652 2308 1026 4091 4062 3947 3487 1647 2479 1709 2728 2707 2623 2286 938 3739 2654 2411 1439 1647 2478 1706 2715 2653 2405 1416 1556 2115 253 999 3981 3622 2186 539 2143 366 1452 1699 2686 2538 1948 3684 2433 1528 2002 3900 3297 886 3530 1818 3163 350 1388 1444 1665 2550 1994 3866 3164 356 1412 1538 2044 4065 3958 3532 1825 3189 455 1805 3111 142 554 2202 603 2399 1390 1451 1695 2670 2474 1691 2653 2405 1413 1543 2062 42 155 608 2419 1469 1766 2953 3605 2119 269 1064 146 569 2264 851 3390 1259 925 3686 2442 1563 2144 371 1469 1765 2949 3590 2059 30 108 419 1661 2536 1939 3645 2278 905 3605 2117 262 1034 26 89 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2790
INFO:transformers.data.processors.glue:input_ids: 2 1532 2017 3959 3533 1830 3209 533 2118 266 1052 99 381 1509 1925 3591 2061 38 138 538 2139 351 1389 1445 1669 2568 2065 53 197 776 3089 54 202 794 3161 341 1350 1290 1049 88 340 1347 1277 998 3980 3617 2168 468 1857 3318 969 3861 3141 263 1037 40 148 577 2293 966 3852 3107 126 490 1947 3679 2414 1450 1691 2653 2406 1418 1563 2142 364 1442 1658 2522 1882 3418 1371 1374 1386 1434 1625 2391 1359 1326 1196 676 2689 2549 1991 3854 3116 164 644 2561 2037 4040 3860 3137 248 977 3893 3272 785 3126 203 799 3184 435 1725 2789 2949 3592 2068 65 245 967 3853 3109 135 525 2087 141 552 2193 568 2258 826 3291 862 3436 1443 1661 2533 1925 3592 2068 67 253 997 3973 3592 2066 59 223 877 3495 1678 2603 2207 624 2481 1718 2764 2849 3190 460 1826 3195 478 1897 3478 1611 2333 1125 392 1553 2102 202 794 3162 347 1374 1387 1437 1640 2449 1591 2253 807 3216 562 2234 732 2916 3460 1538 2044 4066 3963 3551 1901 3493 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-1399
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:input_ids: 2 2484 1732 2817 3064 4051 3903 3311 942 3756 2722 2684 2529 1910 3532 1826 3196 484 1923 3583 2031 4015 3759 2734 2731 2719 2669 2472 1683 2623 2288 948 3779 2815 3053 4008 3731 2622 2283 925 3688 2449 1592 2260 834 3323 990 3948 3491 1664 2546 1979 3808 2931 3520 1778 3002 3803 2910 3436 1444 1665 2552 2004 3908 3331 1023 4079 4013 3752 2707 2621 2280 915 3647 2287 943 3757 2725 2696 2579 2112 243 959 3822 2988 3745 2677 2501 1800 3091 63 239 941 3752 2705 2613 2248 788 3137 247 976 3889 3254 716 2850 3196 484 1923 3582 2026 3996 3683 2430 1515 1952 3699 2496 1780 3009 3832 3028 3906 3322 985 3926 3403 1311 1134 428 1700 2692 2564 2051 4094 4075 3997 3686 2443 1567 2159 432 1715 2751 2800 2995 3773 2792 2961 3637 2245 774 3081 21 71 270 1067 159 622 2475 1695 2670 2475 1695 2670 2475 1695 2670 2474 1691 2654 2411 1439 1647 2479 1709 2727 2701 2598 2186 538 2139 350 1386 1436 1636 2436 1539 2047 4079 4014 3756 2724 2690 2556 2018 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4183
INFO:transformers.data.processors.glue:input_ids: 2 3821 2984 3731 2623 2287 943 3759 2733 2726 2700 2596 2179 510 2028 4001 3703 2511 1838 3243 671 2670 2476 1700 2692 2561 2040 4051 3903 3310 940 3748 2691 2557 2023 3982 3628 2210 635 2526 1897 3480 1620 2371 1277 999 3981 3623 2189 552 2193 567 2254 811 3231 622 2474 1691 2654 2411 1438 1642 2457 1621 2373 1286 1035 30 105 407 1614 2347 1182 619 2463 1647 2479 1710 2731 2718 2667 2462 1642 2459 1630 2410 1434 1625 2392 1363 1341 1253 902 3593 2071 79 301 1189 648 2580 2113 248 980 3908 3330 1020 4065 3959 3535 1838 3244 673 2678 2505 1816 3155 318 1260 931 3710 2539 1949 3688 2452 1603 2302 1003 4000 3700 2499 1790 3052 4001 3701 2504 1812 3137 247 975 3886 3244 676 2689 2552 2004 3907 3326 1003 3998 3692 2467 1662 2540 1953 3701 2503 1806 3115 159 621 2469 1672 2580 2113 248 980 3907 3327 1008 4019 3773 2791 2958 3628 2209 632 2515 1853 3303 910 3628 2211 638 2539 1950 3691 2461 1640 2450 1596 2273 888 3537 1845 3272 787 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2791
INFO:transformers.data.processors.glue:input_ids: 2 1553 2101 200 788 3139 253 998 3978 3610 2138 347 1375 1389 1445 1669 2568 2065 53 197 773 3079 13 38 140 547 2173 488 1937 3637 2248 785 3127 205 807 3216 564 2243 765 3048 3985 3637 2246 779 3102 106 412 1633 2423 1485 1829 3208 529 2101 197 776 3090 60 225 885 3527 1805 3112 145 565 2246 778 3097 88 337 1333 1223 782 3113 152 593 2357 1222 777 3096 82 316 1249 887 3535 1839 3247 688 2740 2753 2808 3025 3896 3284 833 3320 977 3896 3281 824 3281 824 3284 835 3326 1004 4001 3704 2513 1847 3277 808 3218 571 2269 872 3476 1604 2308 1025 4088 4049 3896 3284 835 3326 1004 4001 3704 2513 1847 3277 808 3218 571 2269 872 3476 1604 2308 1025 4088 4049 3896 3284 834 3322 986 3929 3413 1352 1297 1077 197 774 3083 30 105 406 1610 2330 1115 350 1385 1429 1606 2316 1059 125 488 1939 3646 2283 927 3695 2478 1707 2717 2661 2437 1544 2068 65 245 968 3859 3134 234 922 3674 2393 1365 1352 1297 1078 204 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4184
INFO:transformers.data.processors.glue:input_ids: 2 1082 218 860 3428 1409 1526 1994 3868 3171 381 1512 1938 3644 2276 900 3586 2042 4057 3925 3398 1291 1054 108 418 1657 2518 1868 3362 1147 478 1898 3481 1621 2373 1287 1037 40 146 570 2266 858 3419 1373 1381 1413 1542 2059 31 112 434 1722 2779 2911 3438 1450 1690 2650 2394 1370 1371 1373 1384 1428 1602 2299 989 3942 3465 1557 2117 262 1033 21 69 264 1042 58 219 861 3430 1419 1566 2154 409 1621 2376 1300 1091 253 997 3973 3590 2059 29 102 396 1570 2171 477 1893 3464 1553 2104 209 824 3281 822 3276 802 3194 476 1889 3446 1484 1825 3192 467 1853 3302 906 3610 2137 341 1352 1297 1077 198 779 3101 101 392 1553 2103 206 812 3234 635 2526 1897 3478 1612 2337 1142 457 1813 3141 261 1032 18 60 226 890 3548 1889 3445 1480 1809 3128 210 825 3285 840 3346 1081 215 845 3367 1167 557 2214 650 2588 2146 377 1494 1866 3356 1122 379 1501 1894 3467 1566 2154 410 1626 2394 1371 1373 1382 1418 1561 2134 330 1305 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4185
INFO:transformers.data.processors.glue:input_ids: 2 4011 3742 2666 2459 1630 2412 1443 1661 2536 1940 3652 2305 1015 4045 3878 3211 541 2150 395 1565 2152 403 1598 2284 932 3716 2564 2052 4097 4088 4049 3896 3282 828 3298 889 3543 1870 3372 1187 637 2536 1939 3645 2277 904 3604 2116 258 1019 4062 3947 3486 1642 2459 1630 2409 1430 1609 2328 1108 321 1272 978 3899 3294 875 3487 1646 2475 1693 2664 2451 1598 2284 932 3714 2553 2008 3921 3384 1233 823 3278 812 3233 629 2503 1806 3114 155 605 2408 1427 1597 2280 916 3652 2308 1027 4095 4078 4011 3743 2670 2475 1693 2664 2451 1598 2284 932 3714 2556 2020 3969 3576 2004 3907 3326 1004 4001 3701 2503 1806 3114 155 605 2408 1425 1592 2260 836 3332 1028 4099 4095 4078 4011 3743 2670 2475 1693 2664 2451 1598 2284 932 3714 2556 2020 3969 3576 2004 3907 3326 1004 4001 3701 2503 1806 3114 155 605 2408 1427 1600 2292 964 3844 3076 4099 4095 4078 4011 3743 2670 2475 1693 2664 2451 1598 2284 932 3714 2556 2020 3969 3576 2004 3907 3326 1004 4001 3701 2503 1806 3114 155 605 2408 1427 1600 2292 964 3844 3076 4099 4095 4078 4011 3743 2670 2475 1693 2664 2451 1598 2284 932 3714 2556 2020 3969 3576 2004 3907 3326 1004 4001 3701 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2792
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:input_ids: 2 2457 1621 2373 1287 1039 47 173 677 2694 2572 2083 127 495 1965 3752 2705 2615 2255 815 3245 677 2693 2568 2068 68 259 1021 4072 3986 3641 2261 840 3345 1078 201 792 3156 321 1269 965 3847 3085 40 146 572 2274 891 3550 1899 3485 1639 2448 1587 2237 742 2953 3607 2128 308 1218 762 3034 3932 3428 1410 1530 2010 3932 3428 1410 1532 2018 3962 3545 1878 3403 1310 1132 418 1658 2522 1883 3422 1386 1434 1627 2398 1387 1439 1646 2475 1694 2668 2465 1653 2504 1809 3128 210 827 3293 870 3467 1565 2150 395 1565 2150 395 1565 2150 395 1565 2150 395 1565 2150 395 1567 2158 427 1695 2672 2481 1720 2772 2881 3320 980 3908 3332 1025 4088 4049 3896 3283 829 3302 907 3614 2153 406 1612 2340 1156 513 2037 4040 3857 3125 200 785 3125 200 788 3140 260 1027 4093 4071 3982 3628 2209 632 2516 1858 3322 987 3935 3438 1451 1695 2670 2473 1688 2644 2372 1281 1015 4047 3887 3247 686 2729 2712 2641 2358 1226 794 3163 349 1383 1423 1581 2213 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4186
INFO:transformers.data.processors.glue:input_ids: 2 1178 602 2396 1378 1402 1497 1877 3399 1293 1064 147 573 2277 901 3590 2058 25 85 325 1285 1029 5 5 5 5 5 5 6 11 30 105 405 1607 2320 1074 185 726 2890 3354 1114 346 1372 1377 1399 1485 1831 3214 554 2202 602 2394 1370 1371 1374 1386 1435 1629 2407 1421 1576 2196 579 2302 1002 3994 3676 2402 1402 1498 1884 3425 1400 1490 1849 3286 841 3350 1099 286 1130 410 1627 2397 1382 1419 1565 2150 396 1569 2165 453 1797 3078 12 33 118 457 1813 3144 273 1079 206 812 3234 633 2517 1864 3346 1082 218 858 3420 1377 1397 1478 1802 3099 93 358 1420 1571 2174 489 1942 3658 2330 1114 347 1374 1386 1434 1626 2393 1366 1354 1308 1122 377 1493 1862 3338 1050 90 346 1371 1374 1388 1441 1655 2510 1834 3226 604 2404 1409 1525 1992 3858 3130 218 857 3416 1361 1334 1226 795 3166 364 1441 1653 2501 1798 3081 22 74 283 1118 361 1430 1610 2330 1115 350 1388 1441 1655 2511 1838 3242 665 2646 2377 1301 1093 263 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-2793
INFO:transformers.data.processors.glue:input_ids: 2 3650 2299 989 3943 3469 1576 2194 571 2269 871 3471 1581 2215 655 2605 2214 651 2592 2164 451 1790 3049 3990 3660 2340 1156 516 2049 4087 4045 3877 3208 532 2114 252 995 3967 3567 1967 3757 2728 2705 2615 2256 818 3260 740 2948 3586 2043 4064 3956 3524 1793 3061 4040 3857 3127 207 813 3238 651 2592 2163 447 1774 2987 3743 2670 2476 1699 2686 2538 1947 3678 2411 1438 1644 2466 1659 2526 1898 3482 1628 2403 1407 1517 1958 3723 2590 2155 415 1646 2474 1691 2656 2419 1472 1779 3006 3819 2975 3695 2477 1704 2707 2624 2292 962 3833 3032 3924 3394 1276 995 3967 3567 1967 3760 2738 2748 2788 2948 3586 2044 4067 3968 3570 1978 3802 2906 3419 1375 1391 1454 1708 2724 2691 2558 2027 3999 3694 2474 1692 2660 2433 1527 1997 3880 3219 574 2284 932 3716 2564 2050 4091 4063 3950 3500 1700 2692 2564 2050 4092 4068 3971 3582 2028 4003 3709 2535 1936 3635 2239 751 2991 3758 2731 2719 2671 2478 1708 2722 2684 2529 1912 3539 1853 3304 913 3639 2255 815 3245 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-4187
INFO:transformers.data.processors.glue:input_ids: 2 1563 2142 361 1429 1605 2309 1031 15 46 170 666 2650 2393 1367 1357 1319 1166 556 2210 634 2521 1879 3407 1326 1193 661 2630 2314 1050 90 346 1369 1368 1364 1345 1269 965 3846 3082 28 97 374 1484 1826 3196 484 1921 3573 1992 3858 3132 225 888 3540 1857 3317 968 3860 3137 248 977 3894 3274 796 3169 376 1492 1857 3318 972 3875 3198 492 1955 3711 2543 1965 3752 2706 2618 2266 860 3427 1407 1517 1959 3728 2612 2243 767 3056 4017 3766 2763 2847 3182 428 1697 2680 2515 1855 3309 935 3727 2606 2218 666 2652 2404 1409 1525 1992 3857 3126 204 801 3192 467 1855 3309 935 3727 2606 2218 666 2652 2404 1409 1525 1992 3857 3128 211 830 3308 932 3716 2563 2046 4073 3991 3663 2349 1191 653 2598 2187 542 2153 407 1614 2347 1183 621 2472 1683 2624 2289 951 3791 2863 3245 679 2703 2606 2218 666 2650 2396 1377 1397 1480 1810 3130 219 864 3442 1468 1764 2946 3580 2019 3967 3565 1957 3718 2571 2077 104 402 1596 2276 897 3574 1995 3869 3173 392 1554 2106 218 857 3413 1352 1298 1083 221 872 3474 1595 2269 872 3474 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5577
INFO:transformers.data.processors.glue:input_ids: 2 1610 2330 1113 344 1362 1338 1244 867 3455 1517 1960 3730 2620 2275 894 3562 1946 3674 2394 1369 1366 1353 1303 1103 302 1194 666 2652 2401 1398 1482 1818 3163 350 1386 1434 1626 2395 1374 1388 1443 1662 2539 1949 3688 2450 1595 2271 877 3493 1671 2575 2095 175 686 2732 2722 2683 2525 1895 3469 1573 2182 524 2084 129 504 2004 3906 3324 994 3962 3546 1884 3427 1407 1517 1958 3722 2587 2143 367 1456 1716 2754 2812 3044 3972 3585 2038 4044 3873 3191 462 1834 3227 606 2412 1442 1658 2523 1887 3438 1452 1698 2683 2527 1902 3500 1699 2687 2543 1966 3755 2717 2663 2445 1573 2184 530 2108 228 900 3585 2037 4037 3848 3091 63 238 940 3746 2684 2529 1912 3540 1857 3320 980 3906 3322 987 3933 3432 1426 1596 2276 898 3580 2018 3964 3556 1921 3574 1994 3867 3165 360 1427 1599 2287 944 3762 2748 2786 2939 3551 1902 3499 1693 2663 2448 1588 2241 760 3027 3903 3311 941 3752 2706 2620 2275 895 3565 1960 3731 2623 2288 948 3778 2812 3043 3966 3562 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5578
INFO:transformers.data.processors.glue:input_ids: 2 40 147 573 2280 913 3637 2245 773 3078 9 22 75 287 1133 424 1682 2619 2270 875 3487 1646 2476 1698 2683 2527 1902 3499 1696 2675 2496 1778 3003 3808 2929 3510 1737 2838 3145 277 1093 262 1035 31 109 422 1676 2595 2174 492 1953 3704 2513 1845 3270 778 3100 99 381 1512 1940 3649 2293 965 3845 3077 6 12 33 120 467 1853 3304 913 3637 2247 783 3117 166 652 2596 2179 509 2021 3973 3589 2054 10 28 97 373 1479 1805 3111 143 557 2215 654 2601 2199 591 2352 1203 702 2796 2979 3711 2542 1964 3748 2692 2561 2040 4051 3901 3304 913 3637 2245 773 3077 5 5 5 6 10 27 93 360 1425 1592 2260 834 3323 989 3943 3471 1581 2213 647 2573 2086 140 547 2175 494 1963 3743 2672 2483 1727 2799 2989 3749 2694 2571 2077 104 401 1592 2260 833 3319 974 3884 3235 638 2538 1947 3680 2417 1461 1733 2821 3078 9 21 69 264 1041 54 204 803 3198 490 1946 3674 2393 1365 1349 1287 1037 37 134 521 2070 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5579
INFO:transformers.data.processors.glue:input_ids: 2 1582 2220 675 2686 2539 1951 3695 2479 1709 2725 2694 2570 2076 97 373 1480 1810 3132 226 892 3556 1921 3574 1994 3867 3167 367 1454 1707 2717 2664 2452 1604 2305 1013 4040 3859 3134 236 930 3707 2527 1901 3495 1677 2598 2187 542 2156 420 1667 2558 2028 4003 3709 2536 1940 3652 2307 1021 4071 3981 3624 2196 579 2302 1002 3995 3678 2409 1432 1618 2361 1239 846 3371 1182 619 2462 1643 2463 1645 2472 1683 2623 2285 936 3732 2628 2306 1020 4068 3972 3586 2043 4063 3949 3496 1683 2621 2280 915 3646 2283 927 3694 2476 1699 2687 2543 1968 3761 2741 2760 2835 3133 231 909 3623 2189 550 2186 539 2142 363 1438 1641 2455 1615 2349 1191 653 2600 2193 568 2260 834 3324 994 3962 3547 1886 3436 1443 1661 2535 1935 3631 2221 680 2708 2627 2303 1006 4011 3743 2670 2476 1700 2692 2564 2051 4093 4071 3984 3634 2233 728 2900 3395 1279 1005 4007 3725 2600 2195 573 2279 911 3631 2222 681 2709 2629 2309 1031 13 39 141 549 2183 526 2090 153 599 2382 1322 1178 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5580
INFO:transformers.data.processors.glue:input_ids: 2 904 3602 2108 226 892 3555 1918 3561 1943 3661 2342 1161 535 2127 302 1193 662 2635 2333 1126 396 1570 2171 478 1897 3477 1606 2313 1045 69 263 1038 43 158 617 2455 1613 2342 1163 541 2150 396 1570 2171 479 1903 3502 1708 2721 2678 2508 1828 3203 509 2023 3982 3626 2203 606 2410 1433 1621 2374 1292 1059 125 488 1938 3642 2267 861 3431 1421 1575 2189 550 2185 534 2124 292 1155 509 2022 3978 3610 2139 352 1393 1462 1739 2846 3180 418 1660 2532 1922 3577 2007 3917 3365 1160 530 2105 214 844 3361 1143 461 1830 3211 542 2153 405 1605 2309 1029 8 19 61 229 901 3589 2056 19 62 234 921 3669 2373 1286 1034 25 87 334 1322 1178 602 2395 1374 1387 1437 1637 2437 1543 2061 38 140 546 2171 477 1894 3466 1561 2133 326 1292 1059 125 485 1925 3589 2055 13 38 138 539 2143 365 1446 1674 2587 2142 364 1442 1658 2522 1881 3414 1353 1302 1097 278 1098 281 1111 334 1321 1174 588 2337 1143 463 1838 3242 666 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-6971
INFO:transformers.data.processors.glue:input_ids: 2 4039 3855 3119 174 684 2723 2685 2535 1935 3631 2223 686 2731 2719 2671 2477 1703 2703 2607 2223 686 2730 2715 2654 2411 1438 1644 2467 1663 2542 1964 3745 2680 2514 1850 3289 855 3407 1325 1189 646 2570 2073 87 335 1328 1202 698 2778 2905 3413 1349 1288 1044 66 252 996 3970 3580 2020 3970 3580 2018 3963 3550 1900 3490 1659 2528 1908 3524 1793 3064 4049 3895 3279 813 3237 645 2568 2068 66 249 983 3917 3368 1172 577 2296 979 3903 3311 943 3757 2728 2705 2616 2259 829 3304 915 3645 2280 915 3645 2280 914 3643 2271 879 3503 1709 2728 2707 2624 2289 952 3796 2883 3325 998 3979 3615 2158 426 1691 2654 2411 1439 1645 2470 1675 2591 2157 422 1675 2591 2157 422 1676 2595 2174 490 1947 3679 2415 1454 1707 2717 2664 2450 1595 2270 876 3492 1668 2564 2051 4094 4075 3998 3691 2462 1643 2462 1644 2465 1656 2513 1846 3275 799 3181 423 1678 2604 2209 632 2516 1858 3322 987 3934 3436 1442 1660 2532 1923 3582 2027 3999 3695 2478 1706 2713 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-5581
INFO:transformers.data.processors.glue:input_ids: 2 1212 738 2938 3547 1887 3437 1445 1672 2580 2116 260 1025 4087 4045 3880 3218 569 2264 851 3391 1263 943 3758 2732 2723 2686 2539 1952 3700 2499 1791 3054 4010 3739 2656 2417 1464 1746 2874 3291 863 3437 1447 1678 2604 2211 639 2542 1961 3733 2632 2322 1084 228 899 3584 2033 4024 3794 2876 3299 893 3558 1931 3615 2157 423 1678 2603 2207 621 2472 1683 2622 2284 932 3715 2560 2035 4030 3820 2978 3708 2529 1910 3532 1828 3202 508 2020 3971 3583 2031 4015 3760 2737 2743 2768 2865 3256 724 2881 3317 965 3845 3079 14 44 163 640 2548 1986 3833 3030 3916 3364 1156 515 2048 4084 4036 3844 3075 4095 4077 4008 3732 2628 2306 1020 4068 3972 3588 2052 4099 4096 4084 4036 3844 3075 4096 4082 4027 3807 2926 3497 1686 2635 2333 1127 399 1582 2220 674 2683 2527 1903 3502 1708 2724 2692 2563 2046 4075 3999 3695 2479 1711 2733 2728 2708 2626 2300 994 3964 3556 1924 3585 2039 4045 3878 3212 547 2173 488 1938 3644 2273 886 3530 1818 3161 344 1364 1346 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:Writing example 0/1394
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-6972
INFO:transformers.data.processors.glue:input_ids: 2 535 2125 296 1171 573 2277 904 3601 2101 200 785 3125 200 785 3126 202 793 3158 330 1308 1121 374 1482 1818 3162 347 1374 1386 1434 1627 2398 1388 1443 1662 2539 1950 3692 2468 1665 2549 1992 3860 3137 246 970 3867 3165 357 1413 1544 2066 57 213 838 3340 1058 124 482 1913 3541 1861 3333 1030 10 25 86 329 1304 1108 324 1281 1014 4043 3869 3174 396 1572 2177 501 1992 3857 3125 200 786 3130 220 867 3454 1516 1953 3702 2508 1828 3202 506 2009 3925 3397 1286 1035 31 110 426 1689 2645 2376 1299 1085 232 916 3650 2299 989 3942 3467 1566 2155 414 1643 2462 1644 2467 1661 2536 1937 3639 2253 805 3205 517 2053 5 6 11 31 110 428 1697 2677 2504 1812 3138 251 992 3954 3514 1754 2906 3419 1375 1390 1452 1697 2678 2506 1818 3164 353 1399 1486 1836 3234 633 2520 1873 3381 1222 778 3100 97 373 1479 1807 3120 177 694 2764 2852 3204 513 2038 4041 3863 3149 296 1171 575 2286 938 3738 2652 2401 1399 1487 1837 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-8365
INFO:transformers.data.processors.glue:input_ids: 2 624 2483 1727 2798 2987 3742 2666 2459 1629 2407 1421 1573 2182 522 2074 91 351 1389 1448 1684 2627 2304 1011 4031 3824 2994 3771 2781 2920 3476 1603 2302 1003 3997 3686 2443 1565 2152 404 1602 2299 990 3948 3492 1667 2559 2032 4019 3773 2790 2956 3619 2175 495 1965 3752 2708 2626 2299 991 3951 3502 1707 2720 2676 2497 1782 3018 3868 3172 385 1527 1998 3883 3229 615 2445 1576 2196 577 2294 971 3869 3176 403 1597 2280 915 3647 2287 944 3762 2745 2776 2899 3391 1262 938 3738 2652 2403 1408 1524 1986 3833 3032 3922 3387 1248 883 3520 1777 2998 3785 2840 3156 323 1279 1006 4010 3739 2655 2413 1447 1680 2612 2242 762 3034 3932 3425 1398 1483 1822 3179 415 1645 2472 1683 2623 2288 947 3774 2796 2979 3712 2548 1988 3842 3065 4055 3920 3379 1216 755 3006 3819 2974 3691 2464 1652 2498 1788 3043 3968 3572 1986 3833 3031 3917 3368 1171 576 2291 958 3818 2971 3679 2413 1448 1683 2621 2280 916 3651 2301 998 3980 3620 2179 512 2033 4024 3795 2879 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-6973
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:input_ids: 2 3827 3007 3823 2989 3751 2703 2605 2216 657 2613 2248 785 3128 211 832 3315 960 3827 3008 3828 3011 3837 3047 3983 3632 2228 707 2815 3056 4019 3775 2799 2989 3752 2707 2621 2279 911 3632 2227 704 2803 3006 3820 2979 3709 2536 1940 3651 2301 999 3981 3623 2190 555 2208 627 2495 1775 2992 3763 2752 2803 3006 3820 2979 3711 2543 1968 3763 2751 2799 2991 3760 2739 2749 2791 2960 3635 2240 755 3005 3815 2960 3635 2240 755 3007 3823 2992 3763 2752 2803 3006 3819 2976 3699 2495 1776 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3776 2803 3005 3815 2959 3629 2215 655 2606 2220 675 2687 2544 1972 3779 2815 3053 4006 3724 2596 2178 508 2019 3966 3564 1954 3708 2529 1912 3539 1856 3315 960 3827 3007 3823 2992 3763 2752 2803 3007 3823 2991 3759 2736 2739 2751 2800 2996 3780 2819 3071 4079 4015 3760 2739 2752 2803 3008 3827 3006 3819 2975 3694 2476 1700 2692 2562 2043 4063 3951 3503 1712 2740 2755 2813 3047 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:Writing example 0/1396
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-8366
INFO:transformers.data.processors.glue:input_ids: 2 5 5 6 10 26 89 341 1350 1292 1059 125 487 1936 3635 2239 750 2986 3740 2659 2429 1509 1925 3592 2068 67 254 1002 3995 3678 2410 1434 1628 2401 1400 1492 1860 3330 1017 4056 3923 3389 1255 909 3624 2196 577 2296 980 3905 3317 968 3858 3132 227 895 3565 1957 3718 2569 2070 73 280 1106 316 1250 892 3554 1914 3546 1884 3426 1403 1502 1898 3482 1626 2396 1379 1406 1515 1950 3692 2467 1661 2533 1926 3594 2075 94 364 1443 1661 2536 1940 3650 2298 987 3933 3430 1420 1569 2166 460 1826 3194 474 1883 3421 1381 1413 1544 2068 65 245 968 3857 3125 199 783 3117 168 658 2619 2269 871 3471 1582 2218 666 2651 2397 1383 1422 1578 2203 606 2410 1436 1636 2435 1535 2029 4005 3717 2566 2058 26 90 345 1366 1355 1311 1134 428 1697 2680 2513 1845 3270 780 3107 126 492 1953 3701 2504 1809 3128 212 833 3320 979 3902 3308 932 3714 2554 2011 3933 3432 1428 1601 2296 977 3894 3275 797 3175 397 1575 2189 549 2183 525 2086 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-6974
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:input_ids: 2 1924 3587 2045 4071 3983 3630 2217 663 2637 2344 1171 574 2282 922 3676 2401 1399 1488 1844 3267 768 3058 4028 3810 2940 3556 1921 3573 1992 3860 3138 252 993 3960 3539 1854 3305 919 3661 2343 1167 557 2215 655 2606 2218 667 2653 2407 1422 1580 2210 635 2526 1899 3487 1645 2469 1672 2578 2105 215 846 3372 1188 642 2556 2019 3966 3561 1943 3663 2352 1202 699 2782 2924 3490 1658 2524 1890 3451 1503 1901 3495 1680 2611 2238 748 2979 3710 2540 1956 3716 2563 2048 4082 4027 3807 2927 3501 1704 2706 2619 2271 879 3501 1703 2702 2604 2212 643 2559 2031 4014 3756 2723 2686 2539 1950 3692 2468 1668 2564 2051 4094 4074 3995 3679 2414 1452 1698 2682 2523 1888 3443 1471 1774 2988 3747 2685 2534 1931 3614 2155 415 1646 2474 1691 2654 2412 1443 1663 2541 1959 3725 2598 2187 542 2156 420 1668 2563 2048 4084 4034 3836 3044 3970 3580 2019 3967 3565 1958 3724 2595 2173 486 1930 3609 2133 328 1297 1080 211 830 3305 919 3663 2350 1196 673 2678 2507 1824 3185 440 1745 2870 3275 799 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-8367
INFO:transformers.data.processors.glue:input_ids: 2 3172 387 1535 2030 4012 3746 2684 2529 1912 3540 1858 3322 986 3931 3423 1391 1455 1709 2728 2707 2624 2291 959 3823 2991 3759 2734 2732 2724 2689 2552 2002 3898 3290 857 3416 1361 1336 1235 831 3311 942 3756 2721 2677 2502 1804 3107 127 493 1958 3723 2590 2155 415 1647 2478 1706 2715 2654 2411 1439 1645 2471 1679 2607 2223 686 2732 2724 2691 2559 2030 4011 3742 2667 2461 1639 2447 1583 2222 683 2720 2675 2493 1768 2963 3646 2283 927 3693 2471 1678 2602 2204 612 2436 1539 2047 4078 4011 3744 2676 2499 1791 3054 4011 3742 2668 2467 1663 2543 1968 3764 2753 2807 3023 3888 3252 707 2813 3047 3984 3634 2235 735 2925 3496 1683 2621 2280 915 3645 2279 909 3621 2182 523 2078 106 412 1635 2430 1516 1954 3706 2522 1884 3428 1409 1526 1994 3867 3166 364 1444 1667 2558 2028 4002 3706 2523 1886 3434 1436 1635 2430 1513 1944 3665 2360 1236 836 3332 1025 4088 4052 3908 3332 1025 4086 4044 3873 3192 468 1857 3320 977 3893 3269 775 3085 40 148 580 2307 1023 4077 4008 3732 2628 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-6975
INFO:transformers.data.processors.glue:input_ids: 2 1361 1334 1226 793 3158 331 1310 1132 420 1666 2554 2010 3930 3420 1380 1409 1525 1990 3852 3108 131 510 2025 3989 3653 2310 1035 30 106 411 1629 2405 1416 1553 2102 201 791 3150 300 1185 629 2502 1801 3095 77 293 1159 526 2090 153 598 2379 1310 1130 411 1630 2410 1434 1626 2395 1375 1390 1450 1690 2650 2393 1366 1354 1306 1114 345 1366 1354 1305 1109 325 1287 1038 41 151 589 2343 1165 549 2181 518 2057 22 74 284 1121 376 1492 1857 3318 970 3866 3163 350 1388 1444 1665 2552 2004 3906 3322 987 3934 3433 1430 1609 2328 1108 322 1273 981 3912 3346 1082 218 857 3414 1353 1303 1101 294 1164 545 2167 461 1830 3209 534 2124 290 1148 481 1909 3525 1798 3082 26 92 354 1402 1498 1881 3413 1350 1290 1050 89 341 1349 1285 1030 10 25 88 338 1338 1241 853 3399 1293 1061 134 521 2071 78 298 1177 600 2387 1341 1253 904 3602 2107 223 879 3503 1710 2731 2717 2663 2447 1581 2216 659 2621 2277 903 3597 2087 141 551 2189 550 2185 535 2127 301 1190 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-9759
INFO:transformers.data.processors.glue:input_ids: 2 874 3482 1628 2404 1410 1532 2020 3971 3582 2028 4001 3701 2502 1804 3108 131 511 2029 4008 3731 2622 2281 917 3653 2310 1036 36 132 515 2045 4069 3976 3602 2105 216 849 3382 1226 795 3165 357 1416 1556 2113 245 965 3846 3081 21 70 266 1050 89 341 1352 1297 1080 209 822 3275 799 3182 425 1685 2630 2313 1045 69 264 1043 61 231 910 3626 2201 599 2383 1326 1195 669 2662 2444 1571 2175 493 1960 3731 2621 2277 903 3598 2092 162 633 2519 1869 3366 1164 548 2178 505 2006 3913 3350 1097 279 1103 302 1194 667 2654 2411 1438 1644 2465 1653 2504 1811 3135 237 936 3729 2614 2252 801 3192 468 1859 3327 1006 4012 3746 2681 2520 1873 3384 1233 823 3280 818 3257 728 2900 3395 1279 1006 4010 3740 2660 2436 1539 2045 4069 3974 3596 2082 124 482 1914 3547 1885 3432 1427 1597 2280 914 3643 2270 875 3485 1639 2446 1580 2210 634 2521 1880 3410 1337 1237 837 3334 1035 30 107 414 1643 2461 1638 2441 1558 2124 292 1155 509 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-8368
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:input_ids: 2 350 1386 1435 1631 2413 1446 1674 2586 2137 343 1358 1323 1182 619 2462 1643 2461 1638 2441 1559 2126 298 1179 607 2414 1451 1693 2661 2438 1546 2074 89 342 1354 1306 1113 341 1349 1288 1043 62 233 919 3661 2341 1160 531 2112 243 958 3817 2967 3663 2349 1192 658 2619 2269 869 3462 1545 2072 83 317 1253 901 3592 2067 61 229 901 3592 2067 61 229 903 3600 2099 191 749 2983 3725 2600 2196 577 2293 965 3847 3085 37 134 521 2069 69 262 1036 34 122 473 1878 3403 1309 1127 398 1579 2205 615 2446 1578 2202 603 2397 1382 1420 1572 2180 514 2042 4060 3939 3454 1516 1955 3711 2541 1958 3721 2584 2131 319 1261 936 3731 2622 2283 928 3700 2498 1787 3039 3950 3499 1694 2667 2461 1639 2445 1576 2195 575 2285 935 3727 2606 2220 675 2687 2542 1964 3745 2677 2501 1800 3089 53 200 786 3130 220 865 3445 1480 1811 3133 232 916 3651 2303 1007 4014 3755 2717 2664 2449 1591 2253 808 3220 579 2303 1006 4010 3738 2652 2402 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-8369
INFO:transformers.data.processors.glue:input_ids: 2 2535 1935 3629 2216 660 2628 2307 1021 4072 3987 3647 2285 936 3731 2622 2283 927 3694 2476 1699 2686 2540 1954 3708 2530 1915 3549 1896 3475 1597 2279 909 3621 2182 524 2084 132 514 2041 4054 3916 3364 1154 508 2020 3970 3580 2020 3971 3583 2030 4012 3748 2691 2559 2031 4014 3755 2719 2670 2476 1697 2679 2511 1839 3246 683 2720 2676 2499 1791 3056 4017 3767 2765 2856 3220 578 2300 995 3967 3567 1967 3757 2728 2707 2622 2283 927 3696 2483 1727 2798 2988 3745 2680 2514 1850 3292 867 3454 1516 1955 3710 2540 1955 3711 2542 1963 3744 2676 2497 1784 3028 3906 3324 994 3963 3551 1901 3494 1676 2596 2180 515 2046 4075 4000 3699 2495 1776 2996 3778 2812 3043 3967 3566 1964 3747 2686 2539 1951 3693 2472 1684 2628 2306 1019 4064 3953 3512 1745 2872 3281 821 3269 775 3085 39 142 553 2199 590 2346 1177 599 2382 1321 1176 596 2369 1269 968 3860 3139 254 1004 4004 3715 2559 2029 4008 3729 2613 2248 787 3136 244 964 3843 3069 4072 3987 3647 2287 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-9760
INFO:transformers.data.processors.glue:input_ids: 2 383 1518 1964 3746 2681 2518 1867 3357 1126 395 1565 2150 395 1566 2156 418 1658 2521 1879 3406 1323 1181 614 2442 1562 2140 356 1410 1530 2010 3930 3418 1371 1374 1386 1436 1634 2426 1499 1886 3436 1443 1662 2537 1944 3668 2369 1271 975 3887 3246 684 2724 2690 2554 2009 3928 3412 1345 1272 978 3897 3288 849 3381 1222 780 3108 132 513 2039 4045 3877 3206 523 2079 111 432 1713 2744 2769 2872 3281 824 3284 833 3317 965 3848 3091 61 230 908 3617 2166 460 1828 3204 513 2039 4046 3884 3234 636 2530 1913 3542 1866 3354 1114 345 1365 1349 1286 1036 35 125 488 1940 3650 2297 981 3911 3342 1066 154 602 2395 1373 1382 1418 1561 2134 332 1313 1141 454 1803 3101 104 403 1597 2279 912 3636 2242 763 3037 3942 3466 1562 2137 344 1362 1339 1245 869 3464 1553 2102 202 793 3158 329 1302 1098 282 1113 341 1350 1289 1047 78 298 1180 609 2421 1478 1801 3093 72 276 1089 248 980 3908 3329 1014 4044 3876 3201 504 2001 3896 3284 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-9761
INFO:transformers.data.processors.glue:input_ids: 2 807 3215 557 2216 657 2614 2252 801 3191 464 1843 3262 746 2971 3677 2406 1418 1564 2145 376 1490 1850 3292 867 3454 1513 1943 3663 2351 1197 680 2707 2621 2277 901 3591 2064 51 190 748 2978 3705 2519 1870 3370 1179 606 2412 1443 1663 2542 1962 3738 2649 2389 1351 1294 1066 155 605 2408 1425 1589 2246 780 3105 117 455 1806 3115 157 614 2443 1566 2154 410 1626 2396 1377 1397 1477 1800 3091 62 235 926 3690 2459 1629 2406 1418 1562 2140 356 1412 1538 2043 4062 3946 3483 1630 2410 1436 1634 2425 1495 1869 3365 1158 524 2083 126 489 1941 3654 2316 1060 129 504 2001 3893 3269 774 3082 28 98 380 1505 1910 3530 1818 3162 346 1369 1365 1351 1293 1062 138 537 2133 325 1286 1035 29 101 392 1554 2105 213 838 3337 1045 69 262 1035 29 104 401 1591 2253 806 3212 548 2178 506 2009 3926 3402 1307 1119 367 1454 1705 2709 2629 2309 1032 19 61 231 910 3628 2209 631 2511 1837 3239 653 2600 2194 572 2276 898 3580 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-9762
INFO:transformers.data.processors.glue:input_ids: 2 3203 512 2035 4032 3827 3006 3819 2976 3699 2495 1773 2982 3722 2587 2141 360 1427 1597 2279 911 3629 2215 654 2601 2198 588 2340 1156 516 2051 4093 4072 3987 3646 2284 931 3711 2543 1965 3752 2708 2627 2304 1012 4033 3830 3019 3870 3180 420 1665 2549 1989 3845 3079 16 50 187 736 2931 3519 1776 2994 3769 2776 2899 3392 1267 959 3824 2995 3776 2803 3007 3821 2984 3732 2627 2304 1009 4024 3795 2880 3313 949 3783 2832 3121 184 721 2872 3284 834 3324 993 3957 3527 1807 3120 179 703 2800 2995 3775 2800 2995 3776 2802 3002 3802 2907 3423 1392 1457 1718 2763 2845 3176 402 1596 2276 899 3583 2029 4007 3728 2611 2237 741 2949 3591 2064 52 196 771 3071 4079 4016 3764 2756 2820 3076 4099 4096 4083 4032 3828 3009 3831 3023 3888 3252 705 2808 3027 3902 3308 931 3711 2544 1971 3775 2798 2987 3743 2672 2482 1722 2780 2915 3455 1518 1963 3741 2664 2451 1597 2280 915 3647 2288 947 3774 2796 2980 3715 2559 2031 4013 3751 2701 2598 2188 547 2173 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: train-9763
INFO:transformers.data.processors.glue:input_ids: 2 3261 743 2957 3623 2189 552 2196 579 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 135 525 2087 144 564 2244 769 3063 4048 3891 3261 743 2960 3635 2237 744 2964 3651 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 712 2833 3125 199 781 3111 144 564 2244 769 3063 4048 3891 3264 755 3008 3827 3005 3816 2964 3651 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 135 525 2087 144 564 2244 769 3063 4048 3891 3262 747 2976 3699 2493 1768 2964 3651 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 135 525 2087 144 564 2244 769 3063 4048 3891 3264 755 3008 3828 3012 3843 3069 4071 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 135 525 2087 144 564 2244 769 3063 4048 3891 3264 755 3008 3827 3005 3816 2964 3651 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 135 525 2087 144 564 2244 769 3063 4048 3891 3264 755 3008 3827 3005 3816 2964 3651 2301 999 3984 3634 2236 739 2941 3559 1936 3633 2229 711 2829 3109 136 532 2116 257 1015 4048 3891 3264 755 3008 3827 3005 3816 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:__main__:Saving features into cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_train_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 11154
INFO:__main__:  Num Epochs = 15
INFO:__main__:  Instantaneous batch size per GPU = 48
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 48
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 3495
INFO:__main__:  Continuing training from checkpoint, will skip to saved global_step
INFO:__main__:  Continuing training from epoch 0
INFO:__main__:  Continuing training from global step 0
INFO:__main__:  Will skip the first 0 steps in the first epoch
INFO:__main__:Creating features from dataset file at /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/
INFO:transformers.data.processors.glue:Writing example 0/1395
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1
INFO:transformers.data.processors.glue:input_ids: 2 2104 209 824 3284 835 3325 997 3974 3594 2075 93 358 1420 1572 2179 509 2024 3988 3649 2293 968 3859 3136 243 959 3821 2982 3723 2592 2163 447 1773 2982 3724 2596 2180 514 2043 4061 3944 3476 1604 2307 1024 4084 4036 3842 3068 4067 3966 3564 1954 3708 2532 1923 3583 2030 4010 3738 2652 2403 1407 1519 1967 3757 2725 2695 2575 2094 169 663 2639 2351 1199 685 2725 2693 2568 2068 67 255 1007 4013 3749 2693 2565 2056 19 63 240 945 3765 2760 2833 3127 208 817 3254 716 2849 3190 460 1825 3189 456 1810 3131 223 877 3495 1677 2600 2194 572 2274 891 3549 1896 3475 1599 2285 936 3732 2625 2293 967 3853 3110 140 545 2168 466 1849 3287 845 3366 1161 535 2125 295 1168 562 2235 733 2920 3474 1595 2269 871 3472 1588 2243 768 3059 4030 3817 2966 3657 2327 1102 299 1183 624 2484 1732 2818 3068 4068 3969 3574 1996 3873 3192 468 1858 3322 988 3938 3449 1493 1861 3333 1032 17 53 199 782 3116 162 634 2522 1884 3425 1398 1484 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-2
INFO:transformers.data.processors.glue:input_ids: 2 678 2700 2593 2166 457 1813 3144 274 1081 215 847 3373 1190 652 2593 2167 462 1835 3231 621 2469 1670 2572 2081 120 466 1852 3299 895 3567 1965 3752 2708 2628 2307 1024 4082 4027 3807 2925 3496 1684 2627 2301 1000 3988 3651 2304 1010 4027 3807 2928 3506 1722 2779 2909 3429 1415 1549 2088 147 575 2285 934 3723 2591 2158 425 1687 2639 2350 1196 673 2680 2515 1853 3304 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2277 903 3597 2088 147 573 2280 915 3645 2277 903 3597 2088 147 573 2277 903 3597 2088 147 573 2277 903 3597 2088 147 573 2280 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2277 903 3597 2088 147 573 2277 903 3597 2088 147 573 2280 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2280 914 3641 2263 847 3373 1191 653 2598 2187 543 2160 436 1731 2813 3048 3987 3645 2280 915 3645 2280 915 3645 2280 915 3645 2280 915 3645 2280 913 3638 2251 799 3182 428 1699 2688 2548 1988 3842 3065 4053 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-3
INFO:transformers.data.processors.glue:input_ids: 2 2408 1428 1601 2295 976 3891 3261 744 2962 3643 2270 875 3487 1647 2477 1704 2708 2628 2306 1020 4066 3964 3553 1910 3530 1820 3169 375 1487 1837 3237 648 2577 2103 205 806 3212 548 2179 511 2029 4007 3727 2605 2216 660 2627 2302 1004 4001 3701 2504 1811 3135 238 940 3747 2686 2539 1949 3688 2449 1589 2245 776 3091 63 238 938 3739 2653 2406 1417 1560 2132 323 1278 1002 3995 3680 2417 1464 1748 2884 3330 1020 4066 3964 3553 1909 3528 1812 3140 257 1013 4037 3848 3089 53 200 785 3126 204 802 3195 477 1896 3475 1597 2280 915 3645 2280 913 3640 2260 835 3325 1000 3986 3644 2273 885 3525 1800 3089 53 197 773 3077 6 12 36 131 510 2027 3999 3695 2480 1714 2745 2774 2892 3361 1144 468 1859 3326 1004 4004 3715 2559 2029 4008 3731 2624 2291 958 3818 2971 3679 2414 1452 1700 2689 2552 2002 3898 3291 863 3437 1448 1682 2617 2263 847 3374 1195 669 2663 2448 1588 2244 769 3064 4052 3906 3323 990 3946 3484 1634 2426 1500 1889 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-4
INFO:transformers.data.processors.glue:input_ids: 2 3741 2661 2439 1550 2092 163 637 2534 1932 3620 2179 511 2031 4013 3752 2707 2622 2283 926 3691 2461 1639 2447 1583 2221 679 2703 2605 2216 660 2627 2304 1012 4035 3837 3047 3984 3634 2236 737 2933 3528 1809 3126 203 800 3185 440 1747 2877 3303 912 3633 2231 718 2860 3236 643 2560 2035 4029 3816 2963 3645 2279 911 3632 2226 700 2787 2941 3558 1932 3620 2179 512 2035 4031 3824 2994 3772 2785 2933 3528 1810 3130 219 864 3443 1471 1774 2986 3739 2656 2417 1463 1742 2858 3226 602 2394 1371 1374 1386 1435 1631 2415 1453 1702 2698 2587 2143 366 1452 1698 2683 2528 1908 3523 1792 3060 4035 3839 3054 4012 3745 2680 2516 1857 3318 972 3876 3202 507 2015 3952 3507 1727 2797 2983 3728 2609 2232 724 2881 3320 979 3902 3306 923 3679 2413 1448 1682 2620 2275 893 3558 1932 3618 2170 476 1891 3455 1517 1958 3724 2593 2168 468 1859 3327 1006 4010 3738 2651 2399 1391 1455 1709 2728 2706 2620 2274 892 3556 1923 3583 2031 4015 3757 2728 2707 2623 2287 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:Writing example 0/1395
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-5
INFO:transformers.data.processors.glue:input_ids: 2 2044 4065 3960 3539 1853 3304 916 3652 2307 1023 4078 4011 3744 2673 2485 1735 2829 3111 144 564 2243 766 3050 3996 3684 2433 1528 2004 3907 3328 1012 4036 3841 3064 4052 3908 3331 1022 4075 3997 3688 2452 1603 2302 1003 3997 3687 2445 1575 2191 558 2218 668 2658 2427 1503 1903 3503 1709 2728 2707 2623 2288 948 3779 2815 3055 4013 3752 2708 2627 2302 1003 3997 3687 2448 1587 2238 747 2973 3688 2451 1599 2287 941 3750 2699 2591 2157 424 1683 2622 2284 932 3715 2558 2028 4004 3713 2552 2004 3905 3320 980 3906 3324 996 3969 3576 2004 3905 3320 980 3906 3324 996 3969 3576 2004 3908 3332 1026 4092 4067 3968 3569 1976 3796 2883 3325 998 3979 3616 2164 449 1784 3027 3903 3309 936 3731 2624 2291 958 3820 2980 3716 2562 2044 4068 3972 3588 2052 4099 4093 4071 3981 3622 2187 543 2160 436 1732 2820 3073 4086 4041 3863 3151 302 1194 668 2660 2433 1528 2003 3903 3311 943 3758 2732 2724 2689 2552 2004 3905 3320 979 3904 3313 952 3793 2872 3284 836 3331 1021 4072 3987 3648 2292 963 3838 3051 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1396
INFO:transformers.data.processors.glue:input_ids: 2 2691 2558 2028 4002 3706 2522 1883 3422 1386 1435 1631 2413 1448 1681 2613 2246 778 3099 96 370 1466 1756 2915 3453 1511 1936 3635 2237 742 2954 3612 2148 387 1534 2028 4004 3716 2561 2038 4043 3871 3182 427 1695 2671 2479 1711 2736 2739 2751 2799 2990 3755 2719 2669 2472 1683 2623 2286 939 3741 2663 2445 1576 2195 574 2281 918 3658 2331 1118 363 1438 1644 2466 1659 2527 1902 3499 1695 2671 2477 1703 2703 2605 2216 659 2622 2282 923 3677 2406 1420 1570 2169 471 1870 3372 1188 644 2564 2049 4085 4039 3854 3115 159 623 2480 1715 2751 2797 2981 3720 2577 2102 203 797 3173 392 1553 2101 197 776 3092 68 260 1028 4099 4095 4078 4012 3745 2677 2502 1804 3108 130 508 2018 3964 3556 1921 3575 1997 3878 3211 542 2153 407 1614 2347 1184 627 2494 1772 2980 3714 2556 2017 3959 3534 1836 3233 629 2501 1797 3079 13 38 138 539 2141 360 1426 1596 2276 899 3583 2031 4013 3749 2694 2572 2084 131 509 2022 3979 3613 2151 399 1583 2222 681 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1397
INFO:transformers.data.processors.glue:input_ids: 2 3069 4072 3988 3652 2307 1022 4076 4002 3708 2532 1921 3575 1999 3887 3245 678 2699 2591 2159 429 1704 2706 2617 2264 851 3391 1263 943 3757 2727 2704 2611 2237 744 2963 3647 2285 935 3727 2607 2224 691 2751 2798 2987 3743 2671 2477 1702 2699 2589 2152 404 1604 2305 1016 4049 3896 3284 833 3320 979 3902 3307 926 3692 2468 1668 2564 2051 4094 4075 4000 3699 2494 1770 2972 3683 2431 1518 1964 3746 2684 2529 1911 3534 1835 3231 621 2472 1681 2615 2253 807 3214 554 2204 611 2432 1523 1982 3818 2970 3674 2395 1374 1386 1436 1635 2430 1515 1950 3691 2462 1643 2461 1640 2449 1591 2255 815 3247 687 2736 2737 2744 2772 2882 3321 983 3918 3370 1180 612 2435 1533 2021 3974 3596 2081 117 456 1810 3131 223 877 3495 1678 2603 2206 620 2467 1664 2548 1987 3837 3048 3988 3649 2296 979 3901 3304 916 3650 2300 996 3970 3579 2013 3944 3476 1603 2302 1004 4002 3708 2532 1921 3574 1996 3876 3203 511 2029 4007 3728 2611 2240 756 3010 3836 3043 3966 3563 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1398
INFO:transformers.data.processors.glue:input_ids: 2 2653 2405 1415 1549 2087 143 560 2228 707 2816 3060 4035 3840 3060 4035 3840 3059 4031 3824 2995 3776 2804 3012 3843 3072 4084 4035 3839 3055 4015 3760 2740 2755 2816 3057 4023 3792 2865 3255 717 2856 3219 576 2292 962 3836 3044 3971 3584 2036 4035 3840 3060 4035 3840 3060 4035 3840 3059 4032 3828 3012 3843 3072 4084 4035 3840 3060 4035 3840 3060 4035 3840 3060 4035 3840 3060 4035 3840 3060 4035 3838 3051 3999 3694 2475 1696 2674 2491 1758 2922 3483 1631 2415 1455 1712 2740 2755 2815 3056 4019 3773 2789 2951 3599 2094 171 672 2676 2499 1791 3056 4019 3775 2800 2995 3775 2800 2995 3775 2800 2995 3774 2796 2979 3711 2544 1971 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3775 2800 2995 3776 2804 3012 3844 3076 4099 4095 4080 4020 3780 2820 3075 4095 4079 4015 3760 2738 2747 2784 2932 3522 1788 3044 3972 3588 2052 4099 4094 4076 4003 3711 2544 1972 3780 2820 3075 4096 4084 4035 3840 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 0 (id = 0)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1399
INFO:transformers.data.processors.glue:input_ids: 2 794 3162 347 1375 1389 1445 1670 2572 2084 129 501 1989 3845 3080 17 53 197 774 3084 35 126 492 1955 3709 2536 1937 3637 2245 775 3085 38 138 538 2140 356 1411 1533 2023 3981 3622 2186 539 2143 365 1446 1674 2587 2142 362 1433 1623 2383 1325 1189 645 2567 2062 43 158 617 2453 1605 2310 1034 26 90 347 1374 1387 1438 1642 2460 1636 2433 1525 1989 3847 3086 43 159 623 2477 1702 2698 2586 2140 353 1400 1489 1846 3275 797 3174 393 1558 2122 283 1117 358 1417 1558 2122 283 1118 363 1438 1644 2465 1653 2501 1798 3083 29 101 391 1552 2098 187 733 2918 3465 1558 2122 283 1118 363 1438 1644 2465 1653 2501 1798 3083 29 101 391 1552 2098 185 728 2897 3381 1224 786 3129 215 846 3371 1181 614 2442 1561 2134 331 1310 1132 417 1656 2516 1857 3320 979 3903 3312 948 3778 2811 3037 3943 3471 1582 2220 674 2681 2519 1871 3373 1190 651 2590 2156 418 1657 2520 1875 3390 1260 932 3715 2558 2026 3994 3675 2397 1382 1417 1559 2127 302 1193 661 2629 2310 1034 28 99 382 1514 1947 3677 2408 1425 1592 2257 822 3276 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:transformers.data.processors.glue:*** Example ***
INFO:transformers.data.processors.glue:guid: dev-1400
INFO:transformers.data.processors.glue:input_ids: 2 2708 2626 2300 993 3957 3528 1809 3125 200 785 3126 204 804 3204 515 2048 4084 4036 3843 3070 4074 3996 3684 2436 1539 2046 4074 3995 3678 2412 1444 1667 2559 2029 4006 3724 2596 2177 503 2000 3890 3260 739 2943 3567 1968 3761 2744 2772 2881 3320 979 3902 3306 924 3684 2435 1536 2036 4034 3836 3043 3966 3564 1956 3715 2559 2030 4011 3744 2673 2486 1738 2841 3159 335 1326 1196 676 2691 2559 2030 4009 3735 2640 2355 1215 749 2982 3723 2592 2163 447 1773 2982 3724 2596 2177 504 2001 3894 3275 797 3176 403 1599 2288 946 3772 2788 2947 3582 2028 4003 3712 2547 1983 3822 2987 3743 2669 2471 1679 2608 2228 705 2808 3026 3899 3293 870 3467 1565 2150 396 1569 2168 466 1852 3298 891 3549 1893 3463 1549 2085 135 528 2098 188 737 2936 3539 1855 3311 943 3759 2734 2731 2719 2671 2477 1704 2708 2627 2303 1007 4015 3758 2732 2724 2692 2561 2039 4045 3879 3213 551 2192 564 2244 770 3068 4068 3969 3576 2004 3908 3329 1016 4052 3907 3326 1003 3999 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:transformers.data.processors.glue:label: 1 (id = 1)
INFO:__main__:Saving features into cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 2790
INFO:__main__:  Batch size = 48
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6555555555555556
INFO:__main__:  auc = 0.7161372241702926
INFO:__main__:  f1 = 0.6553908235061556
INFO:__main__:  mcc = 0.31453347659493713
INFO:__main__:  precision = 0.6576356589147287
INFO:__main__:  recall = 0.6568986810789013
INFO:__main__:{"eval_acc": 0.6555555555555556, "eval_f1": 0.6553908235061556, "eval_mcc": 0.31453347659493713, "eval_auc": 0.7161372241702926, "eval_precision": 0.6576356589147287, "eval_recall": 0.6568986810789013, "learning_rate": 5.730659025787965e-05, "loss": 0.6594151410460473, "step": 100}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 2790
INFO:__main__:  Batch size = 48
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.671326164874552
INFO:__main__:  auc = 0.72528058129026
INFO:__main__:  f1 = 0.6713038269966318
INFO:__main__:  mcc = 0.34473637809442886
INFO:__main__:  precision = 0.6724814785819062
INFO:__main__:  recall = 0.6722549739235189
INFO:__main__:{"eval_acc": 0.671326164874552, "eval_f1": 0.6713038269966318, "eval_mcc": 0.34473637809442886, "eval_auc": 0.72528058129026, "eval_precision": 0.6724814785819062, "eval_recall": 0.6722549739235189, "learning_rate": 0.0001146131805157593, "loss": 0.6207889038324356, "step": 200}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 2790
INFO:__main__:  Batch size = 48
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.6702508960573477
INFO:__main__:  auc = 0.7198757361417095
INFO:__main__:  f1 = 0.6635409937836092
INFO:__main__:  mcc = 0.3692525531981205
INFO:__main__:  precision = 0.6943633524704691
INFO:__main__:  recall = 0.6753770017730669
INFO:__main__:{"eval_acc": 0.6702508960573477, "eval_f1": 0.6635409937836092, "eval_mcc": 0.3692525531981205, "eval_auc": 0.7198757361417095, "eval_precision": 0.6943633524704691, "eval_recall": 0.6753770017730669, "learning_rate": 0.00017191977077363896, "loss": 0.6324224779009819, "step": 300}
INFO:__main__:Loading features from cached file /data3/linming/DNABERT/examples/data/fold5_100_13944/1/after/cached_dev_6-new-12w-0_300_dnaprom
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 2790
INFO:__main__:  Batch size = 48
INFO:__main__:***** Eval results  *****
INFO:__main__:  acc = 0.5157706093189964
INFO:__main__:  auc = 0.4903795556684905
INFO:__main__:  f1 = 0.34026956727358715
INFO:__main__:  mcc = 0.0
INFO:__main__:  precision = 0.2578853046594982
INFO:__main__:  recall = 0.5
INFO:__main__:{"eval_acc": 0.5157706093189964, "eval_f1": 0.34026956727358715, "eval_mcc": 0.0, "eval_auc": 0.4903795556684905, "eval_precision": 0.2578853046594982, "eval_recall": 0.5, "learning_rate": 0.00019675778766687857, "loss": 0.7304939016699791, "step": 400}
